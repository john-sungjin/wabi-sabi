{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving training\n",
    "\n",
    "Now that basic training works, there's a few things I need to add in:\n",
    "\n",
    "- Attention masking: I need to mask out the attention for the padding tokens. We already should get the attention mask from the data loader, so this should be pretty straightforward\n",
    "- Metrics: I'd like to be able to track loss, FLOPs, memory usage, etc.\n",
    "- Optimization: what training optimizations can I make? E.g. mixed precision training, gradient accumulation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from composer.utils import reproducibility\n",
    "\n",
    "seed = 42\n",
    "reproducibility.seed_all(seed)\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "CACHE_DIR = \"/datadrive/hf_cache\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating Attention Masking\n",
    "\n",
    "I'm realizing the attention masking doesn't really matter?\n",
    "Since it's causal, having padding tokens at the end doesn't affect preceding logits. It won't affect the loss either, since we set those labels to -100.\n",
    "\n",
    "I think it's not worth adding into the code; the is_causal flag is enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikihow (/datadrive/hf_cache/wikihow/all-data_dir=%2Fdatadrive%2Fhf_cache/1.2.0/5343fc81d685acaa086c9cc19eb8706206cd1f8b315792b04c1d7b92091c305e)\n",
      "Loading cached shuffled indices for dataset at /datadrive/hf_cache/wikihow/all-data_dir=%2Fdatadrive%2Fhf_cache/1.2.0/5343fc81d685acaa086c9cc19eb8706206cd1f8b315792b04c1d7b92091c305e/cache-ca61b0a7a4447ccd.arrow\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "import datasets\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "wikihow_data: datasets.Dataset = datasets.load_dataset(\n",
    "    \"wikihow\",\n",
    "    name=\"all\",\n",
    "    data_dir=CACHE_DIR,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    use_auth_token=HF_TOKEN,\n",
    "    split=\"train\",\n",
    "    # streaming=True,\n",
    ").shuffle(\n",
    "    seed=seed\n",
    ")  # type: ignore\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids [220, 2160, 2718, 3914, 254, 6906, 6060, 413, 232, 687, 0, 0, 0, 0, 0]\n",
      "attention_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "seq_len = 6\n",
    "sample = wikihow_data[0][\"text\"][:50]\n",
    "tokenized_sample = tokenizer(\n",
    "    sample,\n",
    "    padding=\"max_length\",\n",
    "    max_length=seq_len,\n",
    ")\n",
    "print(\"input_ids\", tokenized_sample[\"input_ids\"])\n",
    "print(\"attention_mask\", tokenized_sample[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True, False, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "sample_qk = torch.randn(seq_len, seq_len)\n",
    "attn_mask = torch.ones(seq_len, seq_len, dtype=torch.bool).tril(diagonal=0)\n",
    "print(attn_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import WSModel, WSConfig\n",
    "\n",
    "model = WSModel(WSConfig())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "Just gonna paste my whole training script here and mess with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikihow (/datadrive/hf_cache/wikihow/all-data_dir=%2Fdatadrive%2Fhf_cache/1.2.0/5343fc81d685acaa086c9cc19eb8706206cd1f8b315792b04c1d7b92091c305e)\n",
      "Loading cached shuffled indices for dataset at /datadrive/hf_cache/wikihow/all-data_dir=%2Fdatadrive%2Fhf_cache/1.2.0/5343fc81d685acaa086c9cc19eb8706206cd1f8b315792b04c1d7b92091c305e/cache-ca61b0a7a4447ccd.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1b545885224bc9876c5dd9f4039867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/157252 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from typing import Any\n",
    "\n",
    "import datasets\n",
    "import torch.utils.data\n",
    "from composer import Trainer\n",
    "from composer.optim import DecoupledAdamW, LinearWithWarmupScheduler\n",
    "from composer.utils import reproducibility\n",
    "from model import ComposerWSModel, WSConfig\n",
    "from transformers import DataCollatorForLanguageModeling, PreTrainedTokenizerFast\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "CACHE_DIR = \"/datadrive/hf_cache\"\n",
    "\n",
    "###### CONFIG ######\n",
    "model_params = {\n",
    "    \"d_model\": 64,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_layers\": 2,\n",
    "    \"vocab_size\": 8192,\n",
    "}\n",
    "\n",
    "seed = 42\n",
    "optim = {\n",
    "    \"lr\": 1e-4,\n",
    "    \"betas\": (0.9, 0.98),\n",
    "    \"eps\": 1.0e-06,\n",
    "    \"weight_decay\": 1.0e-5,\n",
    "}\n",
    "learning_rate = {\"t_warmup\": \"250ba\", \"alpha_f\": 0.02}\n",
    "precision = \"fp32\"\n",
    "\n",
    "save_folder = \"checkpoints/pretraining/\"\n",
    "save_interval = \"500ba\"\n",
    "hf_save_folder = \"huggingface_model/\"\n",
    "\n",
    "tokenizer_dir = \"tokenizer/\"\n",
    "###### END CONFIG ######\n",
    "\n",
    "\n",
    "reproducibility.seed_all(seed)\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_dir)\n",
    "config = WSConfig(**model_params)\n",
    "\n",
    "text_column_name = \"text\"\n",
    "\n",
    "\n",
    "def tokenize_function(examples: dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Tokenize dataset examples.\n",
    "    \"\"\"\n",
    "    examples[text_column_name] = [\n",
    "        line\n",
    "        for line in examples[text_column_name]\n",
    "        if len(line) > 0 and not line.isspace()\n",
    "    ]\n",
    "    return tokenizer(\n",
    "        examples[text_column_name],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "wikihow_data: datasets.Dataset = datasets.load_dataset(\n",
    "    \"wikihow\",\n",
    "    name=\"all\",\n",
    "    data_dir=CACHE_DIR,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    use_auth_token=HF_TOKEN,\n",
    "    split=\"train\",\n",
    "    # streaming=True,\n",
    ").shuffle(\n",
    "    seed=seed\n",
    ")  # type: ignore\n",
    "\n",
    "tokenized_train = wikihow_data.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=wikihow_data.column_names,  # collate_fn doesn't like other columns\n",
    "    load_from_cache_file=False,\n",
    ")\n",
    "\n",
    "collate_fn = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    tokenized_train, batch_size=64, collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "composer_model = ComposerWSModel(config=config, tokenizer=tokenizer)\n",
    "optimizer = DecoupledAdamW(\n",
    "    composer_model.model.parameters(),\n",
    "    # lr=1.0e-4,\n",
    "    # betas=(0.9, 0.98),\n",
    "    # eps=1.0e-06,\n",
    "    # weight_decay=1.0e-5,\n",
    "    **optim,\n",
    ")\n",
    "lr_scheduler = LinearWithWarmupScheduler(**learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from composer.loggers import WandBLogger\n",
    "from composer.callbacks import SpeedMonitor\n",
    "from composer import Callback, State, Logger, Time\n",
    "\n",
    "\n",
    "class SampleCallback(Callback):\n",
    "    def __init__(\n",
    "        self, sample_prompt: str, tokenizer: PreTrainedTokenizerFast, interval: str\n",
    "    ):\n",
    "        self.sample_prompt_ids = tokenizer.encode(sample_prompt, return_tensors=\"pt\")\n",
    "        self.interval = Time.from_timestring(interval)\n",
    "        self.last_sample = Time(0, \"ba\")\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # create table for samples\n",
    "        self.table = wandb.Table(columns=[\"sample\"])\n",
    "        super().__init__()\n",
    "\n",
    "    def batch_end(self, state: State, logger: Logger):\n",
    "        if (state.timestamp.batch - self.last_sample) < self.interval:\n",
    "            return\n",
    "        output_ids = state.model.generate(\n",
    "            state.device.tensor_to_device(self.sample_prompt_ids),\n",
    "            max_new_tokens=30,\n",
    "        )\n",
    "        output_text = self.tokenizer.decode(output_ids[0])\n",
    "        self.table.add_data(output_text)\n",
    "        logger.log_metrics({\"samples\": self.table})\n",
    "\n",
    "        self.last_sample = state.timestamp.batch\n",
    "\n",
    "\n",
    "wandb_logger = WandBLogger(project=\"wabisabi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/datadrive/wabi-sabi/src/wandb/run-20230627_064617-itdv0cog</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/john-sungjin/wabisabi/runs/itdv0cog' target=\"_blank\">1687848377-loud-armadillo</a></strong> to <a href='https://wandb.ai/john-sungjin/wabisabi' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/john-sungjin/wabisabi' target=\"_blank\">https://wandb.ai/john-sungjin/wabisabi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/john-sungjin/wabisabi/runs/itdv0cog' target=\"_blank\">https://wandb.ai/john-sungjin/wabisabi/runs/itdv0cog</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Config:\n",
      "node_name: unknown because NODENAME environment variable not set\n",
      "num_gpus_per_node: 1\n",
      "num_nodes: 1\n",
      "rank_zero_seed: 2970360314\n",
      "\n",
      "******************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a77701e1f00c41a6aaedd4e060c53a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   0:    0%|| 0/2458 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854094637ee54b278762e4123dd2f0ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.013 MB of 0.013 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss/train/total</td><td>▄▃▄▆▄▆▇▄▄▄▄▄▇▄▄▇▄▇▄▅▄▄▇▁▅▁█▅▆▄▄▇▆▄▅▅▂▆▄▃</td></tr><tr><td>metrics/train/LanguageCrossEntropy</td><td>▄▃▄▆▄▆▇▄▄▄▄▄▇▄▄▇▄▇▄▅▄▄▇▁▅▁█▅▆▄▄▇▆▄▅▅▂▆▄▃</td></tr><tr><td>throughput/batches_per_sec</td><td>▁▁▁▁▁▁██████▇▄▄▄▄▄▄▄▄▄▄▄▄▇████▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>throughput/device/batches_per_sec</td><td>▁▁▁▁▁▁██████▇▄▄▄▄▄▄▄▄▄▄▄▄▇████▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>throughput/device/flops_per_sec</td><td>▁▁▁▁▁▁██████▇▄▄▄▄▄▄▄▄▄▄▄▄▇████▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>throughput/device/mfu</td><td>▁▁▁▁▁▁██████▇▄▄▄▄▄▄▄▄▄▄▄▄▇████▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>throughput/device/samples_per_sec</td><td>▁▁▁▁▁▁██████▇▄▄▄▄▄▄▄▄▄▄▄▄▇████▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>throughput/flops_per_sec</td><td>▁▁▁▁▁▁██████▇▄▄▄▄▄▄▄▄▄▄▄▄▇████▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>throughput/samples_per_sec</td><td>▁▁▁▁▁▁██████▇▄▄▄▄▄▄▄▄▄▄▄▄▇████▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>time/batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/batch_in_epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/epoch</td><td>▁</td></tr><tr><td>time/sample</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/sample_in_epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/token</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/token_in_epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/total</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/train</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/val</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/device_train_microbatch_size</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss/train/total</td><td>5.39131</td></tr><tr><td>metrics/train/LanguageCrossEntropy</td><td>5.39131</td></tr><tr><td>throughput/batches_per_sec</td><td>9.0326</td></tr><tr><td>throughput/device/batches_per_sec</td><td>9.0326</td></tr><tr><td>throughput/device/flops_per_sec</td><td>600389961168.8549</td></tr><tr><td>throughput/device/mfu</td><td>0.07412</td></tr><tr><td>throughput/device/samples_per_sec</td><td>578.08637</td></tr><tr><td>throughput/flops_per_sec</td><td>600389961168.8549</td></tr><tr><td>throughput/samples_per_sec</td><td>578.08637</td></tr><tr><td>time/batch</td><td>430</td></tr><tr><td>time/batch_in_epoch</td><td>430</td></tr><tr><td>time/epoch</td><td>0</td></tr><tr><td>time/sample</td><td>27520</td></tr><tr><td>time/sample_in_epoch</td><td>27520</td></tr><tr><td>time/token</td><td>7045120</td></tr><tr><td>time/token_in_epoch</td><td>7045120</td></tr><tr><td>time/total</td><td>0.01337</td></tr><tr><td>time/train</td><td>0.01337</td></tr><tr><td>time/val</td><td>0.0</td></tr><tr><td>trainer/device_train_microbatch_size</td><td>64</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">1687848377-loud-armadillo</strong> at: <a href='https://wandb.ai/john-sungjin/wabisabi/runs/itdv0cog' target=\"_blank\">https://wandb.ai/john-sungjin/wabisabi/runs/itdv0cog</a><br/>Synced 5 W&B file(s), 8 media file(s), 8 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230627_064617-itdv0cog/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 24\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      2\u001b[0m     model\u001b[39m=\u001b[39mcomposer_model,  \u001b[39m# This is the model from the HuggingFaceModel wrapper class.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     train_dataloader\u001b[39m=\u001b[39mtrain_dataloader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     save_overwrite\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[39m# Start training\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     trainer\u001b[39m.\u001b[39;49mfit()\n\u001b[1;32m     26\u001b[0m     \u001b[39m# Save Hugging Face model\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     config\u001b[39m.\u001b[39msave_pretrained(hf_save_folder)\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/composer/trainer/trainer.py:1804\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, train_dataloader, train_dataloader_label, train_subset_num_batches, spin_dataloaders, duration, reset_time, schedulers, scale_schedule_ratio, step_schedulers_every_batch, eval_dataloader, eval_subset_num_batches, eval_interval, device_train_microbatch_size, precision)\u001b[0m\n\u001b[1;32m   1801\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mscaler \u001b[39m=\u001b[39m ClosureGradScaler() \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_closures() \u001b[39melse\u001b[39;00m GradScaler()\n\u001b[1;32m   1803\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfirst_batch_complete \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1804\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_loop()\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/composer/trainer/trainer.py:1946\u001b[0m, in \u001b[0;36mTrainer._train_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1943\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(dataloader, DataLoader) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(dataloader\u001b[39m.\u001b[39msampler, DistributedSampler):\n\u001b[1;32m   1944\u001b[0m     dataloader\u001b[39m.\u001b[39msampler\u001b[39m.\u001b[39mset_epoch(\u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mtimestamp\u001b[39m.\u001b[39mepoch))\n\u001b[0;32m-> 1946\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mbatch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter_dataloader(TrainerMode\u001b[39m.\u001b[39mTRAIN)):\n\u001b[1;32m   1947\u001b[0m     \u001b[39m# Spin dataloader forward unless dataloader handles internally with dataset_resumption\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspin_dataloaders \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mdataset_resumption \u001b[39mand\u001b[39;00m batch_idx \u001b[39m<\u001b[39m \u001b[39mint\u001b[39m(\n\u001b[1;32m   1949\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mtimestamp\u001b[39m.\u001b[39mbatch_in_epoch):\n\u001b[1;32m   1950\u001b[0m         \u001b[39m# Restore the RNG state immediately before the next batch is yielded from the dataloader\u001b[39;00m\n\u001b[1;32m   1951\u001b[0m         \u001b[39mif\u001b[39;00m batch_idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mtimestamp\u001b[39m.\u001b[39mbatch_in_epoch) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rng_state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/composer/trainer/trainer.py:2942\u001b[0m, in \u001b[0;36mTrainer._iter_dataloader\u001b[0;34m(self, trainer_mode)\u001b[0m\n\u001b[1;32m   2940\u001b[0m     \u001b[39mif\u001b[39;00m trainer_mode \u001b[39m==\u001b[39m TrainerMode\u001b[39m.\u001b[39mTRAIN:\n\u001b[1;32m   2941\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine\u001b[39m.\u001b[39mrun_event(Event\u001b[39m.\u001b[39mBEFORE_DATALOADER)\n\u001b[0;32m-> 2942\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(dataloader_iter)\n\u001b[1;32m   2943\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m   2944\u001b[0m     \u001b[39m# [BEFORE/AFTER]_DATALOADER only runs while training\u001b[39;00m\n\u001b[1;32m   2945\u001b[0m     \u001b[39mif\u001b[39;00m trainer_mode \u001b[39m==\u001b[39m TrainerMode\u001b[39m.\u001b[39mTRAIN:\n\u001b[1;32m   2946\u001b[0m         \u001b[39m# Event.AFTER_DATALOADER is normally called in the train loop. However, if we\u001b[39;00m\n\u001b[1;32m   2947\u001b[0m         \u001b[39m# encounter StopIteration, the train loop will not run. Accordingly, we need to\u001b[39;00m\n\u001b[1;32m   2948\u001b[0m         \u001b[39m# explicitly call the engine to run marker.finish() for the dataloader marker.\u001b[39;00m\n\u001b[1;32m   2949\u001b[0m         \u001b[39m# Otherwise, we will encounter an error at the start of the next epoch when\u001b[39;00m\n\u001b[1;32m   2950\u001b[0m         \u001b[39m# Event.BEFORE_DATALOADER tries to start an unfinished marker.\u001b[39;00m\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset, \u001b[39m\"\u001b[39m\u001b[39m__getitems__\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:2782\u001b[0m, in \u001b[0;36mDataset.__getitems__\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m   2780\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitems__\u001b[39m(\u001b[39mself\u001b[39m, keys: List) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List:\n\u001b[1;32m   2781\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2782\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(keys)\n\u001b[1;32m   2783\u001b[0m     n_examples \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(batch[\u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(batch))])\n\u001b[1;32m   2784\u001b[0m     \u001b[39mreturn\u001b[39;00m [{col: array[i] \u001b[39mfor\u001b[39;00m col, array \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems()} \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_examples)]\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:2778\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2776\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):  \u001b[39m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2777\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2778\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem(key)\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:2763\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2761\u001b[0m formatter \u001b[39m=\u001b[39m get_formatter(format_type, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mfeatures, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mformat_kwargs)\n\u001b[1;32m   2762\u001b[0m pa_subtable \u001b[39m=\u001b[39m query_table(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data, key, indices\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indices \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 2763\u001b[0m formatted_output \u001b[39m=\u001b[39m format_table(\n\u001b[1;32m   2764\u001b[0m     pa_subtable, key, formatter\u001b[39m=\u001b[39;49mformatter, format_columns\u001b[39m=\u001b[39;49mformat_columns, output_all_columns\u001b[39m=\u001b[39;49moutput_all_columns\n\u001b[1;32m   2765\u001b[0m )\n\u001b[1;32m   2766\u001b[0m \u001b[39mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/datasets/formatting/formatting.py:624\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    622\u001b[0m python_formatter \u001b[39m=\u001b[39m PythonFormatter(features\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    623\u001b[0m \u001b[39mif\u001b[39;00m format_columns \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 624\u001b[0m     \u001b[39mreturn\u001b[39;00m formatter(pa_table, query_type\u001b[39m=\u001b[39;49mquery_type)\n\u001b[1;32m    625\u001b[0m \u001b[39melif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcolumn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    626\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m format_columns:\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/datasets/formatting/formatting.py:400\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_column(pa_table)\n\u001b[1;32m    399\u001b[0m \u001b[39melif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 400\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_batch(pa_table)\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/datasets/formatting/formatting.py:443\u001b[0m, in \u001b[0;36mPythonFormatter.format_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlazy:\n\u001b[1;32m    442\u001b[0m     \u001b[39mreturn\u001b[39;00m LazyBatch(pa_table, \u001b[39mself\u001b[39m)\n\u001b[0;32m--> 443\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpython_arrow_extractor()\u001b[39m.\u001b[39;49mextract_batch(pa_table)\n\u001b[1;32m    444\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_features_decoder\u001b[39m.\u001b[39mdecode_batch(batch)\n\u001b[1;32m    445\u001b[0m \u001b[39mreturn\u001b[39;00m batch\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/datasets/formatting/formatting.py:150\u001b[0m, in \u001b[0;36mPythonArrowExtractor.extract_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_batch\u001b[39m(\u001b[39mself\u001b[39m, pa_table: pa\u001b[39m.\u001b[39mTable) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m:\n\u001b[0;32m--> 150\u001b[0m     \u001b[39mreturn\u001b[39;00m pa_table\u001b[39m.\u001b[39mto_pydict()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=composer_model,  # This is the model from the HuggingFaceModel wrapper class.\n",
    "    train_dataloader=train_dataloader,\n",
    "    # eval_dataloader=eval_dataloader,\n",
    "    max_duration=\"1ep\",  # train for more epochs to get better performance\n",
    "    optimizers=optimizer,\n",
    "    schedulers=[lr_scheduler],\n",
    "    device=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    precision=\"fp32\",\n",
    "    progress_bar=True,\n",
    "    loggers=[wandb_logger],\n",
    "    callbacks=[\n",
    "        SpeedMonitor(),\n",
    "        SampleCallback(\"Hi, my name is\", tokenizer, \"50ba\"),\n",
    "    ],\n",
    "    # checkpointing\n",
    "    save_folder=save_folder,\n",
    "    save_filename=\"ep{epoch}-ba{batch}-rank{rank}.pt\",\n",
    "    save_interval=save_interval,\n",
    "    save_overwrite=True,\n",
    ")\n",
    "try:\n",
    "    # Start training\n",
    "    trainer.fit()\n",
    "\n",
    "    # Save Hugging Face model\n",
    "    config.save_pretrained(hf_save_folder)\n",
    "    tokenizer.save_pretrained(hf_save_folder)\n",
    "    composer_model.model.save_pretrained(hf_save_folder)\n",
    "finally:\n",
    "    trainer.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "Wanted to compare Mosaic's estimate with the deepspeed profiler. Looks about right - the MPT estimate is 66 G flops, while the deepspeed estimates 20.03 G flops for forward inference, which means total flops would be 20.03 * 3 = 60.09 G flops. Pretty close!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610624\n",
      "66.46923264 G\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "mpt_estimate = composer_model.flops_per_batch(batch)\n",
    "print(composer_model.n_active_params)\n",
    "print(mpt_estimate / 1e9, \"G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0980e+00, -1.1528e-02, -7.1528e-01,  ...,  1.5902e+00,\n",
       "           1.2393e+00,  1.0311e+00],\n",
       "         [ 7.4352e-01, -1.4786e+00,  3.3844e-01,  ...,  1.5986e+00,\n",
       "          -5.4977e-01, -1.1131e+00],\n",
       "         [ 9.5008e-01, -1.1936e+00, -1.1842e-01,  ..., -3.4778e-01,\n",
       "           3.1346e-01,  1.3806e-01],\n",
       "         ...,\n",
       "         [-3.6953e-02,  5.9130e-01,  6.3312e-01,  ..., -6.7467e-01,\n",
       "           1.1711e+00, -1.8665e-01],\n",
       "         [-1.2335e+00,  2.2180e-01,  6.8240e-01,  ..., -1.5828e-01,\n",
       "          -9.0393e-01, -4.7375e-01],\n",
       "         [ 6.5973e-01,  4.2160e-01, -8.4262e-01,  ..., -1.2005e-01,\n",
       "          -3.9789e-01,  5.5433e-01]],\n",
       "\n",
       "        [[-1.9193e-01, -7.7889e-01,  4.1020e-01,  ...,  8.7949e-01,\n",
       "           7.6157e-01, -1.2559e+00],\n",
       "         [ 3.5590e-01, -9.7230e-01,  3.1907e-01,  ..., -1.5804e+00,\n",
       "          -1.5815e+00, -4.3272e-01],\n",
       "         [-1.0980e+00, -1.1528e-02, -7.1528e-01,  ...,  1.5902e+00,\n",
       "           1.2393e+00,  1.0311e+00],\n",
       "         ...,\n",
       "         [ 7.1277e-01, -2.2877e+00,  4.3686e-01,  ...,  1.0806e-01,\n",
       "           1.3275e+00, -4.1863e-01],\n",
       "         [-1.1362e-01, -7.6255e-01, -8.3364e-01,  ..., -8.5489e-01,\n",
       "           1.5042e+00,  7.1609e-01],\n",
       "         [-4.1291e-01, -1.7300e+00, -6.4006e-01,  ..., -1.3567e+00,\n",
       "          -5.2061e-01,  3.4690e-01]],\n",
       "\n",
       "        [[ 9.1169e-01, -2.1514e+00, -5.7449e-01,  ..., -2.9918e-01,\n",
       "          -4.8054e-01,  8.1013e-01],\n",
       "         [-1.7040e+00,  8.8942e-01,  2.7634e-01,  ..., -4.4354e-01,\n",
       "           1.0497e+00,  8.8926e-01],\n",
       "         [-1.5689e+00, -1.1515e+00, -1.0705e-01,  ...,  2.3685e-01,\n",
       "          -9.5182e-01, -4.5237e-01],\n",
       "         ...,\n",
       "         [ 2.8332e-01, -2.0793e-01,  5.3289e-01,  ...,  1.8034e+00,\n",
       "           1.1254e+00,  1.8468e-01],\n",
       "         [-2.4604e+00, -2.8126e-01,  2.4597e+00,  ...,  3.1986e-02,\n",
       "           1.2023e+00, -3.7289e-01],\n",
       "         [ 8.1321e-01,  2.2483e-01,  2.9953e-01,  ...,  1.4426e+00,\n",
       "          -1.2381e+00,  2.0790e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 8.1321e-01,  2.2483e-01,  2.9953e-01,  ...,  1.4426e+00,\n",
       "          -1.2381e+00,  2.0790e-01],\n",
       "         [-2.1598e+00, -2.5319e+00, -2.4515e+00,  ...,  1.6278e+00,\n",
       "           3.3929e-01, -1.8120e+00],\n",
       "         [ 8.7907e-02,  7.3106e-01,  5.1090e-01,  ...,  9.0888e-01,\n",
       "           1.8439e+00,  5.1943e-01],\n",
       "         ...,\n",
       "         [-1.1361e+00, -1.1130e+00,  1.6512e+00,  ..., -1.0518e-01,\n",
       "          -4.8942e-01, -1.9736e+00],\n",
       "         [-1.1361e+00, -1.1130e+00,  1.6512e+00,  ..., -1.0518e-01,\n",
       "          -4.8942e-01, -1.9736e+00],\n",
       "         [-1.1361e+00, -1.1130e+00,  1.6512e+00,  ..., -1.0518e-01,\n",
       "          -4.8942e-01, -1.9736e+00]],\n",
       "\n",
       "        [[ 4.8071e-01, -1.7694e+00,  7.0750e-01,  ...,  9.1336e-01,\n",
       "           3.0543e-01,  5.6635e-02],\n",
       "         [ 4.3200e-01,  3.6622e-01,  2.9215e+00,  ..., -3.5052e-01,\n",
       "          -1.8450e-01,  1.8383e+00],\n",
       "         [ 3.1996e-02, -9.3292e-01, -1.3948e+00,  ..., -1.4074e+00,\n",
       "          -1.0812e-01,  2.7902e-01],\n",
       "         ...,\n",
       "         [ 4.6799e-01, -2.0546e+00, -3.6393e-01,  ...,  1.3694e+00,\n",
       "           2.2447e-01,  7.7585e-01],\n",
       "         [ 6.6864e-01,  1.5483e+00,  1.2140e+00,  ..., -2.0154e+00,\n",
       "           2.0812e+00,  3.8715e-01],\n",
       "         [ 3.3191e-01, -1.1434e+00,  2.8706e-01,  ...,  4.7930e-01,\n",
       "          -1.1064e+00,  2.5995e-01]],\n",
       "\n",
       "        [[-2.9127e-01,  1.1602e+00,  1.2354e+00,  ...,  9.7506e-02,\n",
       "          -4.3250e-01,  8.9703e-01],\n",
       "         [ 4.1365e-01, -1.3331e+00, -2.9540e-01,  ...,  5.6800e-01,\n",
       "          -4.0983e-01, -9.9730e-01],\n",
       "         [ 3.3191e-01, -1.1434e+00,  2.8706e-01,  ...,  4.7930e-01,\n",
       "          -1.1064e+00,  2.5995e-01],\n",
       "         ...,\n",
       "         [ 1.3088e+00,  5.7905e-01,  5.7377e-01,  ..., -1.2820e+00,\n",
       "           1.3974e+00,  7.8925e-01],\n",
       "         [ 6.1847e-02,  2.3352e+00,  1.0211e-01,  ...,  3.9881e-01,\n",
       "          -2.0909e-01,  6.5620e-01],\n",
       "         [-4.3358e-01, -9.0980e-01,  1.1175e+00,  ..., -2.1903e+00,\n",
       "          -8.1773e-04,  4.1227e-01]]], device='cuda:0',\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = torch.nn.Embedding(\n",
    "    num_embeddings=config.vocab_size, embedding_dim=config.d_model\n",
    ").to(\"cuda\")\n",
    "emb(batch[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cpu\n",
      "torch.int64\n",
      "cpu\n",
      "torch.int64\n",
      "\n",
      "-------------------------- DeepSpeed Flops Profiler --------------------------\n",
      "Profile Summary at step 1:\n",
      "Notations:\n",
      "data parallel size (dp_size), model parallel size(mp_size),\n",
      "number of parameters (params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (flops), floating-point operations per second (FLOPS),\n",
      "fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n",
      "step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n",
      "\n",
      "params per gpu:                                               610.62 k\n",
      "params of model = params per GPU * mp_size:                   0       \n",
      "fwd MACs per GPU:                                             10.0 GMACs\n",
      "fwd flops per GPU:                                            20.03 G \n",
      "fwd flops of model = fwd flops per GPU * mp_size:             20.03 G \n",
      "fwd latency:                                                  109.0 ms\n",
      "fwd FLOPS per GPU = fwd flops per GPU / fwd latency:          183.78 GFLOPS\n",
      "\n",
      "----------------------------- Aggregated Profile per GPU -----------------------------\n",
      "Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n",
      "depth 0:\n",
      "    params      - {'WSModel': '610.62 k'}\n",
      "    MACs        - {'WSModel': '10.0 GMACs'}\n",
      "    fwd latency - {'WSModel': '109.0 ms'}\n",
      "depth 1:\n",
      "    params      - {'Embedding': '524.29 k'}\n",
      "    MACs        - {'Linear': '8.59 GMACs'}\n",
      "    fwd latency - {'Linear': '77.58 ms'}\n",
      "depth 2:\n",
      "    params      - {'WSBlock': '86.27 k'}\n",
      "    MACs        - {'WSBlock': '1.41 GMACs'}\n",
      "    fwd latency - {'WSBlock': '30.04 ms'}\n",
      "depth 3:\n",
      "    params      - {'FusedMLP': '65.54 k'}\n",
      "    MACs        - {'FusedMLP': '1.07 GMACs'}\n",
      "    fwd latency - {'FusedMLP': '17.29 ms'}\n",
      "depth 4:\n",
      "    params      - {'Sequential': '65.54 k'}\n",
      "    MACs        - {'Sequential': '1.07 GMACs'}\n",
      "    fwd latency - {'Sequential': '17.22 ms'}\n",
      "depth 5:\n",
      "    params      - {'Linear': '65.54 k'}\n",
      "    MACs        - {'Linear': '1.07 GMACs'}\n",
      "    fwd latency - {'Linear': '11.77 ms'}\n",
      "\n",
      "------------------------------ Detailed Profile per GPU ------------------------------\n",
      "Each module profile is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n",
      "\n",
      "WSModel(\n",
      "  610.62 k, 100.00% Params, 10.0 GMACs, 100.00% MACs, 109.0 ms, 100.00% latency, 183.78 GFLOPS, \n",
      "  (tokens_to_embeddings): Embedding(524.29 k, 85.86% Params, 0 MACs, 0.00% MACs, 244.38 us, 0.22% latency, 0.0 FLOPS, 8192, 64)\n",
      "  (blocks): ModuleList(\n",
      "    (0): WSBlock(\n",
      "      43.14 k, 7.06% Params, 704.64 MMACs, 7.05% MACs, 14.98 ms, 13.74% latency, 95.05 GFLOPS, \n",
      "      (layer_norm_before_attention): FusedLayerNorm(64, 0.01% Params, 0 MACs, 0.00% MACs, 354.77 us, 0.33% latency, 14.78 GFLOPS, )\n",
      "      (attention): WSMultiQueryAttention(\n",
      "        10.24 k, 1.68% Params, 167.77 MMACs, 1.68% MACs, 5.18 ms, 4.75% latency, 64.76 GFLOPS, \n",
      "        (residual_to_qkv): Linear(6.14 k, 1.01% Params, 100.66 MMACs, 1.01% MACs, 1.35 ms, 1.24% latency, 149.38 GFLOPS, in_features=64, out_features=96, bias=False)\n",
      "        (concat_attention_to_residual): Linear(4.1 k, 0.67% Params, 67.11 MMACs, 0.67% MACs, 783.44 us, 0.72% latency, 171.32 GFLOPS, in_features=64, out_features=64, bias=False)\n",
      "      )\n",
      "      (layer_norm_before_ffn): FusedLayerNorm(64, 0.01% Params, 0 MACs, 0.00% MACs, 362.87 us, 0.33% latency, 14.45 GFLOPS, )\n",
      "      (ffn): FusedMLP(\n",
      "        32.77 k, 5.37% Params, 536.87 MMACs, 5.37% MACs, 8.46 ms, 7.76% latency, 127.36 GFLOPS, \n",
      "        (mlp): Sequential(\n",
      "          32.77 k, 5.37% Params, 536.87 MMACs, 5.37% MACs, 8.43 ms, 7.73% latency, 127.92 GFLOPS, \n",
      "          (0): Linear(16.38 k, 2.68% Params, 268.44 MMACs, 2.68% MACs, 2.56 ms, 2.35% latency, 210.0 GFLOPS, in_features=64, out_features=256, bias=False)\n",
      "          (1): FusedDropoutBias(\n",
      "            0, 0.00% Params, 0 MACs, 0.00% MACs, 1.86 ms, 1.71% latency, 2.25 GFLOPS, \n",
      "            (activation_pytorch): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 1.04 ms, 0.95% latency, 4.04 GFLOPS, approximate='none')\n",
      "          )\n",
      "          (2): Linear(16.38 k, 2.68% Params, 268.44 MMACs, 2.68% MACs, 3.64 ms, 3.34% latency, 147.31 GFLOPS, in_features=256, out_features=64, bias=False)\n",
      "          (3): FusedDropoutBias(\n",
      "            0, 0.00% Params, 0 MACs, 0.00% MACs, 255.35 us, 0.23% latency, 0.0 FLOPS, \n",
      "            (activation_pytorch): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 16.45 us, 0.02% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): WSBlock(\n",
      "      43.14 k, 7.06% Params, 704.64 MMACs, 7.05% MACs, 15.06 ms, 13.81% latency, 94.56 GFLOPS, \n",
      "      (layer_norm_before_attention): FusedLayerNorm(64, 0.01% Params, 0 MACs, 0.00% MACs, 373.13 us, 0.34% latency, 14.05 GFLOPS, )\n",
      "      (attention): WSMultiQueryAttention(\n",
      "        10.24 k, 1.68% Params, 167.77 MMACs, 1.68% MACs, 4.92 ms, 4.52% latency, 68.15 GFLOPS, \n",
      "        (residual_to_qkv): Linear(6.14 k, 1.01% Params, 100.66 MMACs, 1.01% MACs, 1.21 ms, 1.11% latency, 166.45 GFLOPS, in_features=64, out_features=96, bias=False)\n",
      "        (concat_attention_to_residual): Linear(4.1 k, 0.67% Params, 67.11 MMACs, 0.67% MACs, 787.02 us, 0.72% latency, 170.54 GFLOPS, in_features=64, out_features=64, bias=False)\n",
      "      )\n",
      "      (layer_norm_before_ffn): FusedLayerNorm(64, 0.01% Params, 0 MACs, 0.00% MACs, 367.4 us, 0.34% latency, 14.27 GFLOPS, )\n",
      "      (ffn): FusedMLP(\n",
      "        32.77 k, 5.37% Params, 536.87 MMACs, 5.37% MACs, 8.83 ms, 8.10% latency, 122.08 GFLOPS, \n",
      "        (mlp): Sequential(\n",
      "          32.77 k, 5.37% Params, 536.87 MMACs, 5.37% MACs, 8.79 ms, 8.07% latency, 122.6 GFLOPS, \n",
      "          (0): Linear(16.38 k, 2.68% Params, 268.44 MMACs, 2.68% MACs, 2.36 ms, 2.17% latency, 227.04 GFLOPS, in_features=64, out_features=256, bias=False)\n",
      "          (1): FusedDropoutBias(\n",
      "            0, 0.00% Params, 0 MACs, 0.00% MACs, 2.83 ms, 2.60% latency, 1.48 GFLOPS, \n",
      "            (activation_pytorch): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 1.54 ms, 1.41% latency, 2.73 GFLOPS, approximate='none')\n",
      "          )\n",
      "          (2): Linear(16.38 k, 2.68% Params, 268.44 MMACs, 2.68% MACs, 3.21 ms, 2.94% latency, 167.3 GFLOPS, in_features=256, out_features=64, bias=False)\n",
      "          (3): FusedDropoutBias(\n",
      "            0, 0.00% Params, 0 MACs, 0.00% MACs, 271.8 us, 0.25% latency, 0.0 FLOPS, \n",
      "            (activation_pytorch): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 16.69 us, 0.02% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer_norm_final): FusedLayerNorm(64, 0.01% Params, 0 MACs, 0.00% MACs, 863.08 us, 0.79% latency, 6.07 GFLOPS, )\n",
      "  (embeddings_to_logits): Linear(524.29 k, 85.86% Params, 8.59 GMACs, 85.91% MACs, 77.58 ms, 71.17% latency, 221.45 GFLOPS, in_features=64, out_features=8192, bias=False)\n",
      ")\n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from deepspeed.profiling.flops_profiler import get_model_profile\n",
    "\n",
    "batch = next(iter(train_dataloader)).to(\"cuda\")\n",
    "batch_size = 64\n",
    "test_composer_model = ComposerWSModel(config=config, tokenizer=tokenizer)\n",
    "print(test_composer_model.model.device)\n",
    "flops, macs, params = get_model_profile(\n",
    "    model=test_composer_model.model,\n",
    "    input_shape=(batch_size, 256),\n",
    "    # kwargs={\"input_ids\": batch[\"input_ids\"]},\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
