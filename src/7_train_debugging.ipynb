{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging\n",
    "\n",
    "There's a bug somewhere - the model doesn't seem to be improving at all. I'll be trying to find it. What's the best way to do this?\n",
    "\n",
    "First, I'll check my inputs. Maybe something's wrong with the training data + labels. To verify, I'll start by pretraining a tiny model implementation that I know works.\n",
    "\n",
    "The tokenization appears fine."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss\n",
    "\n",
    "Trying to see if there's any issue with the labels and batch data. Can't find anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikihow (/datadrive/hf_cache/wikihow/all-data_dir=%2Fdatadrive%2Fhf_cache/1.2.0/5343fc81d685acaa086c9cc19eb8706206cd1f8b315792b04c1d7b92091c305e)\n",
      "Loading cached shuffled indices for dataset at /datadrive/hf_cache/wikihow/all-data_dir=%2Fdatadrive%2Fhf_cache/1.2.0/5343fc81d685acaa086c9cc19eb8706206cd1f8b315792b04c1d7b92091c305e/cache-ca61b0a7a4447ccd.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb0308568df45e9869dd5d0f3fb38e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/157252 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Any\n",
    "from transformers import DataCollatorForLanguageModeling, PreTrainedTokenizerFast\n",
    "import torch\n",
    "import datasets\n",
    "from composer.utils import reproducibility\n",
    "\n",
    "seed = 42\n",
    "reproducibility.seed_all(seed)\n",
    "\n",
    "CACHE_DIR = \"/datadrive/hf_cache\"\n",
    "tokenizer_dir = \"tokenizer/\"\n",
    "\n",
    "context_length = 256\n",
    "batch_size = 128\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_dir)\n",
    "\n",
    "wikihow_data: datasets.Dataset = datasets.load_dataset(\n",
    "    \"wikihow\",\n",
    "    name=\"all\",\n",
    "    data_dir=CACHE_DIR,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    split=\"train\",\n",
    "    # streaming=True,\n",
    ").shuffle(\n",
    "    seed=seed\n",
    ")  # type: ignore\n",
    "\n",
    "text_column_name = \"text\"\n",
    "\n",
    "\n",
    "def tokenize_function(examples: dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Tokenize dataset examples.\n",
    "    \"\"\"\n",
    "    examples[text_column_name] = [\n",
    "        line\n",
    "        for line in examples[text_column_name]\n",
    "        if len(line) > 0 and not line.isspace()\n",
    "    ]\n",
    "    return tokenizer(\n",
    "        examples[text_column_name],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized_train = wikihow_data.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=wikihow_data.column_names,  # collate_fn doesn't like other columns\n",
    "    # load_from_cache_file=False,\n",
    ")\n",
    "\n",
    "collate_fn = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    tokenized_train, batch_size=batch_size, collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The general characteristics of cats vary from breed to breed, and even within breeds, cats vary from one another just like humans do. Don’t choose a name before you get your cat, because it certainly is not a “one name fits all” situation. Some cats are very vocal and active, while others are quiet and lazy. Some want to be in your lap all day, while others may prefer its own space.Consider these traits while you are considering names.\\n\\n\\nThe name Lord Paddington IV is an excellent name, but it may work best for a calm, reserved cat. The name Spazzy would be great for a hyperactive, silly cat.\\nGive your cat some time to acclimate to your home. It may seem timid and quiet at first, but really just be going through a period of adjustment.;\\n, While of course there is more to your precious cat than its physical appearance, it’s a great way to generate some unique potential names. If you can find one that describes its appearance while also fitting its personality, you’ve struck gold!\\n\\n\\nYou may want to go with a dark, mysterious name like Midnight, Raven, or Shadow for a black cat, while an orange cat might make a perfect Ginger or Pumpkin. Longer haired, fluffy cats might make a great Fuzz or Puff.\\n Learn a little bit about the history of its breed to come up with a unique (and informed!) name. For example, you could give your Persian cat a beautiful Persian name like Ali or Zahra.Because legend states that Birman cats are the “Sacred Cat of Burma,” you could brainstorm heavenly names like Goddess or Angel., A name that\\'s cute or makes you laugh isn’t necessarily right for the creature you’ve just brought home. For example, a regal Siamese cat will not feel respected if you name them Poopsie, and in turn it will lose respect for you. A hairless cat will likely feel self-conscious and disrespected if you name it Fluffy. Consider your feline’s dignity when choosing its lifelong label.\\n\\n\\nDepression can occur in cats just as it does in humans. A depressed cat lose its appetite, avoids its owners, and becomes more sedentary. Don’t make your cat sad., Read it out loud. Which names flow off the tongue? Some names may seem more appealing on paper.Look at your cat while you read the potential names. Does your cat just look like one of the names? Get the opinions of family and friends. Sometimes you hear the ideal name and it just clicks. Other times, you will need to slowly eliminate your options one by one.\\n\\n\\nRemember, you’ll be saying this name a lot. Pick a name that you enjoy saying.\\nContemplate the possible nicknames. You may love the name Benedict, but you\\'ll likely start calling your kitty Ben or Benny for convenience. If you choose a long name for your cat, it helps when it’s easily shortened to a nickname.\\nOn that note, keep in mind that cats do respond best to short names.Penelope is great, but Penny might be even better.\\n This is a great way to find a name for your new cat. Think about your favorite movies, television shows, and books. Is there a superhero you\\'ve always idolized or a powerful heroine that you admire? What about that underrated minor character in your favorite novel? There is a wealth of special, meaningful names at your fingertips.\\n For example, if you\\'re getting a cat with your significant other think about where you had your first date, where you were married, or any other special memories you\\'ve made. If you honeymooned in Maui, why not name your cat Maui? If you\\'re a flower lover, think about names like Daisy or Rose. You can honor your beloved mother by naming your cat after her birth month. Is your favorite song \"Sweet Caroline\"? Perfect, that\\'s a great name! Truly, your options are endless. Start brainstorming the things that are special to you, and run with it.Not only will you give your cat a name with love and originality, but you\\'ll also have an interesting story to tell your guests when they ask you your cats name!\\n\\n, Skim through the pages, read the meanings of names, and highlight the names that stick out to you. Once you\\'ve read through the book and highlighted your favorite names, put it away for awhile. Come back to it later with fresh eyes, and put a star next to your favorite highlighted names. These are the best of the best! Keep narrowing your list until you\\'ve only got two or three left. Give yourself some time to think about your top options, and whichever one you can\\'t get out of your mind is the one you should choose.\\n\\n\\nBe original. When you’ve generated some options, think about how many people or animals you know with those names. Your cat’s name certainly doesn’t have to be revolutionary, but don’t you want to give it a name that’s as special as it is?\\n\\n, It won’t give you an answer in English, but it might voice its approval through a purr, some body language, or through its sweet, feline eyes. Call to it in different names and see if any get a reaction. Cats are clever and strong-willed, and it should have some say in this important decision., Use it when you feed, pet, cuddle, and praise your cat. Help it learn its name by using it more than feels natural. Cats are very reward-motivated, so say its name while you give it treats.Cats are often very independent, so don’t feel discouraged if it doesn\\'t always come when you call. It may just be playing hard-to-get.\\nTry to make sure that your cat has only positive associations with its name. Don’t yell its name when it\\'s done something wrong, but sweetly say its name when it\\'s done something right.\\n If you have decided that its full name must be Sir William Fluffy-Butt but you know that you will often call it Willy, get it used to hearing Willy. If you alternate names every time you call it, it’ll likely be confused. If it\\'s confused, it will probably start ignoring you all together. It’s not at all uncommon for a cat to give its human caretakers the cold shoulder, so help your odds by making sure it knows its name., This is the final, and most official, step in the naming process. It can wear its new jewelry with pride, and it is your cats official introduction to the world. Even better, this new identification can be used if your sweet feline family member gets lost. Its collar can help to bring it safely back home.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikihow_data[0][\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the general characteristics of cats vary from breed to breed, and even within breeds, cats vary from one another just like humans do. don’t choose a name before you get your cat, because it certainly is not a “one name fits all” situation. some cats are very vocal and active, while others are quiet and lazy. some want to be in your lap all day, while others may prefer its own space.consider these traits while you are considering names.\\n\\n\\nthe name lord paddington iv is an excellent name, but it may work best for a calm, reserved cat. the name spazzy would be great for a hyperactive, silly cat.\\ngive your cat some time to acclimate to your home. it may seem timid and quiet at first, but really just be going through a period of adjustment.;\\n, while of course there is more to your precious cat than its physical appearance, it’s a great way to generate some unique potential names. if you can find one that describes its appearance while also fitting its personality, you’ve struck gold!\\n\\n\\nyou may want to go with a dark, mysterious name like'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_train[0][\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = iter(train_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1696,  306,  638,  ..., 1347, 3841, 2022],\n",
      "        [6790, 3246,  790,  ...,  540,  260, 1075],\n",
      "        [ 421, 1648,   12,  ...,  220, 3202,  758],\n",
      "        ...,\n",
      "        [2496, 7058, 6778,  ..., 8133,   26,  288],\n",
      "        [ 317,  258,  391,  ...,  220, 2249,  254],\n",
      "        [1732,  220, 6287,  ...,    0,    0,    0]])\n",
      "tensor([[1696,  306,  638,  ..., 1347, 3841, 2022],\n",
      "        [6790, 3246,  790,  ...,  540,  260, 1075],\n",
      "        [ 421, 1648,   12,  ...,  220, 3202,  758],\n",
      "        ...,\n",
      "        [2496, 7058, 6778,  ..., 8133,   26,  288],\n",
      "        [ 317,  258,  391,  ...,  220, 2249,  254],\n",
      "        [1732,  220, 6287,  ..., -100, -100, -100]])\n"
     ]
    }
   ],
   "source": [
    "sample = next(train_data)\n",
    "print(sample[\"input_ids\"])\n",
    "print(sample[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 306,  638, 1610,  ..., 3841, 2022, -100],\n",
      "        [3246,  790, 3729,  ...,  260, 1075, -100],\n",
      "        [1648,   12,  313,  ..., 3202,  758, -100],\n",
      "        ...,\n",
      "        [7058, 6778,   39,  ...,   26,  288, -100],\n",
      "        [ 258,  391, 1180,  ..., 2249,  254, -100],\n",
      "        [ 220, 6287,  467,  ..., -100, -100, -100]])\n",
      "tensor([ 306,  638, 1610,  ..., -100, -100, -100])\n"
     ]
    }
   ],
   "source": [
    "# checking to see if the rolling works as intended\n",
    "from einops import rearrange\n",
    "\n",
    "labels = sample[\"labels\"]\n",
    "# shift labels left\n",
    "labels = torch.roll(labels, -1, dims=1)\n",
    "labels[:, -1] = -100  # don't predict the last token\n",
    "print(labels)\n",
    "# flatten\n",
    "labels = rearrange(labels, \"batch seq_len -> (batch seq_len)\")\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Either FairScale or torch distributed is not available, MixtureOfExperts will not be exposed. Please install them if you would like to use MoE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt IDs: tensor([[ 220, 1178,  254]])\n",
      "Output IDs: tensor([[ 220, 1178,  254,  220, 1178,  254,  220, 1178,  254,  220, 1178,  254,\n",
      "          220, 1178,  254,  220, 1178,  254,  220, 1178,  254,  220, 1178]])\n",
      "Output Text:  the top of the top of the top of the top of the top of the top of the top of the top\n"
     ]
    }
   ],
   "source": [
    "from model import WSConfig, WSModel\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "hf_model_dir = \"huggingface_model/\"\n",
    "\n",
    "test_config = WSConfig.from_pretrained(hf_model_dir)\n",
    "test_model = WSModel.from_pretrained(hf_model_dir, config=test_config)\n",
    "test_tokenizer = PreTrainedTokenizerFast.from_pretrained(hf_model_dir)\n",
    "\n",
    "prompt = \"the top of\"\n",
    "prompt_ids = test_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "print(\"Prompt IDs:\", prompt_ids)\n",
    "\n",
    "\n",
    "output_ids = test_model.generate(prompt_ids, max_new_tokens=20)\n",
    "print(\"Output IDs:\", output_ids)\n",
    "\n",
    "output_text = test_tokenizer.decode(output_ids[0])\n",
    "print(\"Output Text:\", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = test_model(sample[\"input_ids\"])\n",
    "output_logits = rearrange(\n",
    "    outputs.logits,\n",
    "    \"batch seq_len vocab_size -> (batch seq_len) vocab_size\",\n",
    "    vocab_size=8192,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ising,gress.oneyment who a good of thekerud, and you hairop of a. you can a the a good in you to day have, the top.\\n on the youor. theising, can to the ableer. the and you be able thatwork and. and. and a ifet.\\n\\n, the. the theising, and.. and aate.\\n out of. theing the and. andate. andwork of and theosh.\\n\". theet. a hourinkical. be be able for a who in the. aies of\\n, buttonam. and areshes.. and be a.. the aally. aize and\\n is beu the the good. thes and.. andwork and awork of\\n, andified. a,. you aerses. thety.s.\\n ofising,als, not be aified.\\n, and you can ale. and good,.. aified. theising, the media. be you can good. to you the ifship.ly.ments\\n toper. the with azer and, and be, able it is\\n.ship., aoney.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = torch.argmax(outputs.logits[1, :, :], dim=1)\n",
    "tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 256, 8192])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32768, 8192])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32768])\n"
     ]
    }
   ],
   "source": [
    "print(labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second batch, first token logits tensor([-3.5652,  2.6967,  3.3603,  ..., -0.8174, -0.5433, -4.8314],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Rearranged logits tensor([-3.5652,  2.6967,  3.3603,  ..., -0.8174, -0.5433, -4.8314],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# logit rearranging seems fine\n",
    "print(\"Second batch, first token logits\", outputs.logits[1, 0, :])\n",
    "print(\"Rearranged logits\", output_logits[256, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3246,  790, 3729,  260,  477,  898,  346,  208, 4597,  254,  458,   48,\n",
      "         704,   12,  602,  313,  380, 1925,  391, 2140,  317,  258,  391, 6720,\n",
      "         346,  208, 4475,  358, 3760,  692,   58, 2621,  251,  220, 1979,   14,\n",
      "        3973, 4708,  991, 4843,  217,  254, 6790, 3246,  258,  871,  251,  318,\n",
      "        4406,  237,  238,   12,  358, 1426,  318, 3410, 1304,   12, 2849,   12,\n",
      "        2025,   12,  326, 1197, 3543, 2076,   14,  352,  154, 7057, 2509,  239,\n",
      "         238, 6790, 3246,   12, 3868, 1928,   12,  326, 4089,  639,   14, 7394,\n",
      "         509, 4224,  238, 3627,  239,   12, 6115,   12, 4089,  639,   12, 1304,\n",
      "        2375,   12,  261, 2622, 2870,   14, 4064,   57,  238, 3543, 2076,  326,\n",
      "         247,  312, 1272, 2076,  404,  612,  318, 5446,  284, 2094, 5629,  238,\n",
      "        6585,  308, 5155, 1172,   14,  154, 1200,  673, 4460, 6518,  809, 1904,\n",
      "         497, 5117,   57,   12,  720,  501, 4393,   57,  238, 2128, 4178,  534,\n",
      "         326, 5336, 5313,   14,  440, 1426, 4040,  311,  251,  208, 2509,  238,\n",
      "        1904,   12, 8053, 2849,   12, 1304,   12,  326, 1304, 2375, 1914,  154,\n",
      "          12, 2007, 5812,  391, 4000, 4838,  358, 2665, 1885, 1563, 3313,  306,\n",
      "        2315, 1249, 3371,   57,   14, 1022, 6790, 3246, 2725,  858,  473,  387,\n",
      "         501, 2007, 5812,   14, 1548,   12,  317,  258,  391, 6985, 2100,   12,\n",
      "         208, 4735, 2372, 1745,  326, 2007, 2092,  238, 6790, 3246,  261, 2611,\n",
      "        3528,  404, 1714,  258,  208, 1853,  560,  667, 6720,  824, 1877, 4848,\n",
      "        6962,  316, 1706,   14,  220, 2080, 1835,  254, 2743,  284, 2036,   12,\n",
      "        1548,   12,  453, 1829,  318, 5574,  329,   14, 5101, 1877,   57, 3893,\n",
      "         540,  260, 1075, -100])\n",
      "tensor([3246,  790, 3729,  260,  477,  898,  346,  208, 4597,  254,  458,   48,\n",
      "         704,   12,  602,  313,  380, 1925,  391, 2140,  317,  258,  391, 6720,\n",
      "         346,  208, 4475,  358, 3760,  692,   58, 2621,  251,  220, 1979,   14,\n",
      "        3973, 4708,  991, 4843,  217,  254, 6790, 3246,  258,  871,  251,  318,\n",
      "        4406,  237,  238,   12,  358, 1426,  318, 3410, 1304,   12, 2849,   12,\n",
      "        2025,   12,  326, 1197, 3543, 2076,   14,  352,  154, 7057, 2509,  239,\n",
      "         238, 6790, 3246,   12, 3868, 1928,   12,  326, 4089,  639,   14, 7394,\n",
      "         509, 4224,  238, 3627,  239,   12, 6115,   12, 4089,  639,   12, 1304,\n",
      "        2375,   12,  261, 2622, 2870,   14, 4064,   57,  238, 3543, 2076,  326,\n",
      "         247,  312, 1272, 2076,  404,  612,  318, 5446,  284, 2094, 5629,  238,\n",
      "        6585,  308, 5155, 1172,   14,  154, 1200,  673, 4460, 6518,  809, 1904,\n",
      "         497, 5117,   57,   12,  720,  501, 4393,   57,  238, 2128, 4178,  534,\n",
      "         326, 5336, 5313,   14,  440, 1426, 4040,  311,  251,  208, 2509,  238,\n",
      "        1904,   12, 8053, 2849,   12, 1304,   12,  326, 1304, 2375, 1914,  154,\n",
      "          12, 2007, 5812,  391, 4000, 4838,  358, 2665, 1885, 1563, 3313,  306,\n",
      "        2315, 1249, 3371,   57,   14, 1022, 6790, 3246, 2725,  858,  473,  387,\n",
      "         501, 2007, 5812,   14, 1548,   12,  317,  258,  391, 6985, 2100,   12,\n",
      "         208, 4735, 2372, 1745,  326, 2007, 2092,  238, 6790, 3246,  261, 2611,\n",
      "        3528,  404, 1714,  258,  208, 1853,  560,  667, 6720,  824, 1877, 4848,\n",
      "        6962,  316, 1706,   14,  220, 2080, 1835,  254, 2743,  284, 2036,   12,\n",
      "        1548,   12,  453, 1829,  318, 5574,  329,   14, 5101, 1877,   57, 3893,\n",
      "         540,  260, 1075, -100])\n"
     ]
    }
   ],
   "source": [
    "test_labels = torch.roll(sample[\"labels\"], -1, dims=1)\n",
    "test_labels[:, -1] = -100  # don't predict the last token\n",
    "print(test_labels[1, :])\n",
    "print(labels[256:512])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate\n",
    "\n",
    "Maybe the generate function is a little weird? Going to implement greedy decoding myself and see if there are differences.\n",
    "\n",
    "Nope, it seems to be working fine.\n",
    "\n",
    "It seems to be either my model implementation or how its being trained; to be sure, I think it's worth to train on a reliable model implementation and replicate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt: str):\n",
    "    input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    output = test_model(input)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input[\"input_ids\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 20\n",
    "prompt = \"First, you must\"\n",
    "\n",
    "curr_str = prompt\n",
    "for i in range(max_tokens):\n",
    "    input_ids = tokenizer(curr_str, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    output = test_model(input_ids)\n",
    "    pred_token_id = output.logits[0, -1, :].argmax()\n",
    "    new_token = tokenizer.decode(pred_token_id)\n",
    "    curr_str += new_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_output = test_model.generate(\n",
    "    tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"], max_new_tokens=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' first, you must be able to the top of the top of the top of the top of the top of the top'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(generate_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First, you must be able to the top of the top of the top of the top of the top of the top'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_str"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same Data, Different Model\n",
    "\n",
    "If I train a reliable model implementation on the same data but with different parameters and the results are similar, I can just try to scale up the model and see if it solves any issues.\n",
    "\n",
    "They recommend using Docker images, so I'll be doing that.\n",
    "\n",
    "Setup was a little annoying. For whatever reason, installing Docker Desktop doesn't properly install the service, so I used Docker Engine. Then, to access GPUs inside the container, you need to install Nvidia Container Toolkit.\n",
    "\n",
    "The command to run is:\n",
    "```\n",
    "sudo docker run -it --runtime=nvidia --gpus all -v /datadrive:/datadrive mosaicml/llm-foundry:2.0.1_cu118-latest /bin/bash\n",
    "```\n",
    "\n",
    "Actually... I'll keep this around, but I don't feel like plumbing the depths of the llm-foundry code. I'll just adapt my training script to use GPT-Neo-X or something.\n",
    "\n",
    "Ok, so it clearly trains with Neo-X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any\n",
    "\n",
    "import datasets\n",
    "import torch.utils.data\n",
    "import torchinfo\n",
    "import wandb\n",
    "from composer import Callback, Logger, State, Time, Trainer\n",
    "from composer.callbacks import SpeedMonitor\n",
    "from composer.loggers import WandBLogger\n",
    "from composer.optim import DecoupledAdamW, LinearWithWarmupScheduler\n",
    "from composer.utils import reproducibility\n",
    "from composer.models.huggingface import HuggingFaceModel\n",
    "from transformers import DataCollatorForLanguageModeling, PreTrainedTokenizerFast\n",
    "import transformers\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "CACHE_DIR = \"/datadrive/hf_cache\"\n",
    "\n",
    "seed = 42\n",
    "reproducibility.seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# editing config to match mine (mostly)\n",
    "# just want a general sense of the model to be the same\n",
    "\n",
    "\n",
    "name = \"EleutherAI/gpt-neox-20b\"\n",
    "config = transformers.GPTNeoXConfig.from_pretrained(name, cache_dir=CACHE_DIR)\n",
    "\n",
    "context_length = 256\n",
    "batch_size = 128\n",
    "config.hidden_size = 256\n",
    "config.intermediate_size = 1024\n",
    "config.num_attention_heads = 4\n",
    "config.num_hidden_layers = 6\n",
    "config.vocab_size = 8192\n",
    "\n",
    "optim = {\n",
    "    \"lr\": 6.0e-4,\n",
    "    \"betas\": (0.9, 0.95),\n",
    "    \"eps\": 1.0e-08,\n",
    "    \"weight_decay\": 0.0,\n",
    "}\n",
    "learning_rate = {\"t_warmup\": \"100ba\", \"alpha_f\": 0.1}\n",
    "\n",
    "model = transformers.GPTNeoXForCausalLM(config=config)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(name, cache_dir=CACHE_DIR)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"tokenizer/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikihow (/datadrive/hf_cache/wikihow/all-data_dir=%2Fdatadrive%2Fhf_cache/1.2.0/5343fc81d685acaa086c9cc19eb8706206cd1f8b315792b04c1d7b92091c305e)\n",
      "Loading cached shuffled indices for dataset at /datadrive/hf_cache/wikihow/all-data_dir=%2Fdatadrive%2Fhf_cache/1.2.0/5343fc81d685acaa086c9cc19eb8706206cd1f8b315792b04c1d7b92091c305e/cache-ca61b0a7a4447ccd.arrow\n",
      "Loading cached processed dataset at /datadrive/hf_cache/wikihow/all-data_dir=%2Fdatadrive%2Fhf_cache/1.2.0/5343fc81d685acaa086c9cc19eb8706206cd1f8b315792b04c1d7b92091c305e/cache-2206b29dbc9b948e.arrow\n"
     ]
    }
   ],
   "source": [
    "text_column_name = \"text\"\n",
    "\n",
    "\n",
    "def tokenize_function(examples: dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Tokenize dataset examples.\n",
    "    \"\"\"\n",
    "    examples[text_column_name] = [\n",
    "        line\n",
    "        for line in examples[text_column_name]\n",
    "        if len(line) > 0 and not line.isspace()\n",
    "    ]\n",
    "    return tokenizer(\n",
    "        examples[text_column_name],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "wikihow_data: datasets.Dataset = datasets.load_dataset(\n",
    "    \"wikihow\",\n",
    "    name=\"all\",\n",
    "    data_dir=CACHE_DIR,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    use_auth_token=HF_TOKEN,\n",
    "    split=\"train\",\n",
    "    # streaming=True,\n",
    ").shuffle(\n",
    "    seed=seed\n",
    ")  # type: ignore\n",
    "\n",
    "tokenized_train = wikihow_data.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=wikihow_data.column_names,  # collate_fn doesn't like other columns\n",
    "    # load_from_cache_file=False,\n",
    ")\n",
    "\n",
    "collate_fn = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    tokenized_train, batch_size=batch_size, collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from composer.metrics.nlp import LanguageCrossEntropy\n",
    "\n",
    "train_metrics = [LanguageCrossEntropy()]\n",
    "composer_model = HuggingFaceModel(\n",
    "    model=model,\n",
    "    shift_labels=True,\n",
    "    tokenizer=tokenizer,\n",
    "    use_logits=True,\n",
    "    metrics=train_metrics,\n",
    ")\n",
    "torchinfo.summary(\n",
    "    composer_model.model, input_size=(batch_size, context_length), dtypes=[torch.long]\n",
    ")\n",
    "\n",
    "optimizer = DecoupledAdamW(\n",
    "    composer_model.model.parameters(),\n",
    "    **optim,\n",
    ")\n",
    "lr_scheduler = LinearWithWarmupScheduler(**learning_rate)\n",
    "\n",
    "\n",
    "class SampleCallback(Callback):\n",
    "    def __init__(\n",
    "        self, sample_prompt: str, tokenizer: PreTrainedTokenizerFast, interval: str\n",
    "    ):\n",
    "        self.sample_prompt_ids = tokenizer.encode(sample_prompt, return_tensors=\"pt\")\n",
    "        self.interval = Time.from_timestring(interval)\n",
    "        self.last_sample = Time(0, \"ba\")\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # create table for samples\n",
    "        self.table = wandb.Table(columns=[\"sample\"])\n",
    "        super().__init__()\n",
    "\n",
    "    def batch_end(self, state: State, logger: Logger):\n",
    "        if (state.timestamp.batch - self.last_sample) < self.interval:\n",
    "            return\n",
    "        output_ids = state.model.generate(\n",
    "            state.device.tensor_to_device(self.sample_prompt_ids),\n",
    "            max_new_tokens=30,\n",
    "        )\n",
    "        output_text = self.tokenizer.decode(output_ids[0])\n",
    "        self.table.add_data(output_text)\n",
    "        logger.log_metrics({\"samples\": self.table})\n",
    "\n",
    "        self.last_sample = state.timestamp.batch\n",
    "\n",
    "\n",
    "wandb_logger = WandBLogger(project=\"wabisabi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjohn-sungjin\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/datadrive/wabi-sabi/src/wandb/run-20230628_021123-6l711hur</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/john-sungjin/wabisabi/runs/6l711hur' target=\"_blank\">1687918282-maroon-pig</a></strong> to <a href='https://wandb.ai/john-sungjin/wabisabi' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/john-sungjin/wabisabi' target=\"_blank\">https://wandb.ai/john-sungjin/wabisabi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/john-sungjin/wabisabi/runs/6l711hur' target=\"_blank\">https://wandb.ai/john-sungjin/wabisabi/runs/6l711hur</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-28 02:11:23,723] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Config:\n",
      "node_name: unknown because NODENAME environment variable not set\n",
      "num_gpus_per_node: 1\n",
      "num_nodes: 1\n",
      "rank_zero_seed: 789680082\n",
      "\n",
      "******************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07df816208504a3c8320ec2ec996734f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   0:    0%|| 0/1229 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1ba\n",
      "2ba\n",
      "3ba\n",
      "4ba\n",
      "5ba\n",
      "6ba\n",
      "7ba\n",
      "8ba\n",
      "9ba\n",
      "10ba\n",
      "1ba\n",
      "2ba\n",
      "3ba\n",
      "4ba\n",
      "5ba\n",
      "6ba\n",
      "7ba\n",
      "8ba\n",
      "9ba\n",
      "10ba\n",
      "1ba\n",
      "2ba\n",
      "3ba\n",
      "4ba\n",
      "5ba\n",
      "6ba\n",
      "7ba\n",
      "8ba\n",
      "9ba\n",
      "10ba\n",
      "1ba\n",
      "2ba\n",
      "3ba\n",
      "4ba\n",
      "5ba\n",
      "6ba\n",
      "7ba\n",
      "8ba\n",
      "9ba\n",
      "10ba\n",
      "1ba\n",
      "2ba\n",
      "3ba\n",
      "4ba\n",
      "5ba\n",
      "6ba\n",
      "7ba\n",
      "8ba\n",
      "9ba\n",
      "10ba\n",
      "1ba\n",
      "2ba\n",
      "3ba\n",
      "4ba\n",
      "5ba\n",
      "6ba\n",
      "7ba\n",
      "8ba\n",
      "9ba\n",
      "10ba\n",
      "1ba\n",
      "2ba\n",
      "3ba\n",
      "4ba\n",
      "5ba\n",
      "6ba\n",
      "7ba\n",
      "8ba\n",
      "9ba\n",
      "10ba\n",
      "1ba\n",
      "2ba\n",
      "3ba\n",
      "4ba\n",
      "5ba\n",
      "6ba\n",
      "7ba\n",
      "8ba\n",
      "9ba\n",
      "10ba\n",
      "1ba\n",
      "2ba\n",
      "3ba\n",
      "4ba\n",
      "5ba\n",
      "6ba\n",
      "7ba\n",
      "8ba\n",
      "9ba\n",
      "10ba\n",
      "1ba\n",
      "2ba\n",
      "3ba\n",
      "4ba\n",
      "5ba\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss/train/total</td><td>████▇▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>metrics/train/LanguageCrossEntropy</td><td>████▇▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>time/batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/batch_in_epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/epoch</td><td>▁</td></tr><tr><td>time/sample</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/sample_in_epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/token</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/token_in_epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/total</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/train</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/val</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/device_train_microbatch_size</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss/train/total</td><td>5.76012</td></tr><tr><td>metrics/train/LanguageCrossEntropy</td><td>5.76012</td></tr><tr><td>time/batch</td><td>95</td></tr><tr><td>time/batch_in_epoch</td><td>95</td></tr><tr><td>time/epoch</td><td>0</td></tr><tr><td>time/sample</td><td>12160</td></tr><tr><td>time/sample_in_epoch</td><td>12160</td></tr><tr><td>time/token</td><td>3112960</td></tr><tr><td>time/token_in_epoch</td><td>3112960</td></tr><tr><td>time/total</td><td>0.0211</td></tr><tr><td>time/train</td><td>0.0211</td></tr><tr><td>time/val</td><td>0.0</td></tr><tr><td>trainer/device_train_microbatch_size</td><td>128</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">1687918282-maroon-pig</strong> at: <a href='https://wandb.ai/john-sungjin/wabisabi/runs/6l711hur' target=\"_blank\">https://wandb.ai/john-sungjin/wabisabi/runs/6l711hur</a><br/>Synced 5 W&B file(s), 9 media file(s), 9 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230628_021123-6l711hur/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 30\u001b[0m\n\u001b[1;32m      6\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      7\u001b[0m     model\u001b[39m=\u001b[39mcomposer_model,  \u001b[39m# This is the model from the HuggingFaceModel wrapper class.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     train_dataloader\u001b[39m=\u001b[39mtrain_dataloader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     save_overwrite\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m     \u001b[39m# Start training\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     trainer\u001b[39m.\u001b[39;49mfit()\n\u001b[1;32m     31\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     trainer\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/composer/trainer/trainer.py:1804\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, train_dataloader, train_dataloader_label, train_subset_num_batches, spin_dataloaders, duration, reset_time, schedulers, scale_schedule_ratio, step_schedulers_every_batch, eval_dataloader, eval_subset_num_batches, eval_interval, device_train_microbatch_size, precision)\u001b[0m\n\u001b[1;32m   1801\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mscaler \u001b[39m=\u001b[39m ClosureGradScaler() \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_closures() \u001b[39melse\u001b[39;00m GradScaler()\n\u001b[1;32m   1803\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfirst_batch_complete \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1804\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_loop()\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/composer/trainer/trainer.py:1979\u001b[0m, in \u001b[0;36mTrainer._train_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1976\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mlog_metrics({\u001b[39m'\u001b[39m\u001b[39mtime/token\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mtimestamp\u001b[39m.\u001b[39mtoken\u001b[39m.\u001b[39mvalue})\n\u001b[1;32m   1977\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mlog_metrics({\u001b[39m'\u001b[39m\u001b[39mtime/token_in_epoch\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mtimestamp\u001b[39m.\u001b[39mtoken_in_epoch\u001b[39m.\u001b[39mvalue})\n\u001b[0;32m-> 1979\u001b[0m total_loss_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_batch(use_grad_scaling)\n\u001b[1;32m   1981\u001b[0m \u001b[39mif\u001b[39;00m use_grad_scaling:\n\u001b[1;32m   1982\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mupdate()\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/composer/trainer/trainer.py:2163\u001b[0m, in \u001b[0;36mTrainer._train_batch\u001b[0;34m(self, use_grad_scaling)\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mstep(optimizer,\n\u001b[1;32m   2160\u001b[0m                                    closure\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m loss_dict\u001b[39m=\u001b[39mtotal_loss_dict, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m   2161\u001b[0m                                    _train_microbatches(microbatches, loss_dict, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs))\n\u001b[1;32m   2162\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2163\u001b[0m             optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m loss_dict\u001b[39m=\u001b[39;49mtotal_loss_dict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_microbatches(\n\u001b[1;32m   2164\u001b[0m                 microbatches, loss_dict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39;49mitem())\n\u001b[1;32m   2165\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2166\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_microbatches(microbatches, total_loss_dict)\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     68\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/composer/optim/decoupled_weight_decay.py:288\u001b[0m, in \u001b[0;36mDecoupledAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[0;32m--> 288\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    290\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[1;32m    291\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/composer/trainer/trainer.py:2163\u001b[0m, in \u001b[0;36mTrainer._train_batch.<locals>.<lambda>\u001b[0;34m(loss_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mstep(optimizer,\n\u001b[1;32m   2160\u001b[0m                                    closure\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m loss_dict\u001b[39m=\u001b[39mtotal_loss_dict, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m   2161\u001b[0m                                    _train_microbatches(microbatches, loss_dict, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs))\n\u001b[1;32m   2162\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2163\u001b[0m             optimizer\u001b[39m.\u001b[39mstep(closure\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m loss_dict\u001b[39m=\u001b[39mtotal_loss_dict, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_microbatches(\n\u001b[1;32m   2164\u001b[0m                 microbatches, loss_dict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39mitem())\n\u001b[1;32m   2165\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2166\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_microbatches(microbatches, total_loss_dict)\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/composer/trainer/trainer.py:2266\u001b[0m, in \u001b[0;36mTrainer._train_microbatches\u001b[0;34m(self, microbatches, total_loss_dict, ddp_sync)\u001b[0m\n\u001b[1;32m   2264\u001b[0m \u001b[39mfor\u001b[39;00m microbatch_idx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mbatch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(microbatches):\n\u001b[1;32m   2265\u001b[0m     is_final_microbatch \u001b[39m=\u001b[39m microbatch_idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(microbatches)\n\u001b[0;32m-> 2266\u001b[0m     microbatch_loss_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_microbatch(use_grad_scaling, current_batch_size, is_final_microbatch)\n\u001b[1;32m   2268\u001b[0m     \u001b[39m# Aggregate each loss in microbatch_loss_dict into total_loss_dict\u001b[39;00m\n\u001b[1;32m   2269\u001b[0m     \u001b[39mfor\u001b[39;00m k, microbatch_loss \u001b[39min\u001b[39;00m microbatch_loss_dict\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/composer/trainer/trainer.py:2393\u001b[0m, in \u001b[0;36mTrainer._train_microbatch\u001b[0;34m(self, use_grad_scaling, current_batch_size, is_final_microbatch)\u001b[0m\n\u001b[1;32m   2390\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2391\u001b[0m     \u001b[39m# Scale loss based on the number of samples in the microbatch to maintain gradient numerics\u001b[39;00m\n\u001b[1;32m   2392\u001b[0m     microbatch_loss\u001b[39m.\u001b[39mmul_(microbatch_num_samples \u001b[39m/\u001b[39m current_batch_size)\n\u001b[0;32m-> 2393\u001b[0m     microbatch_loss\u001b[39m.\u001b[39;49mbackward(create_graph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backwards_create_graph)\n\u001b[1;32m   2395\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine\u001b[39m.\u001b[39mrun_event(Event\u001b[39m.\u001b[39mAFTER_BACKWARD)\n\u001b[1;32m   2397\u001b[0m \u001b[39m# Use microbatch outputs to update training metrics\u001b[39;00m\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_folder = \"gpt-neox/checkpoints/pretraining\"\n",
    "save_interval = \"500ba\"\n",
    "hf_save_folder = \"gpt-neox/huggingface_model/\"\n",
    "\n",
    "# Create Trainer Object\n",
    "trainer = Trainer(\n",
    "    model=composer_model,  # This is the model from the HuggingFaceModel wrapper class.\n",
    "    train_dataloader=train_dataloader,\n",
    "    # eval_dataloader=eval_dataloader,\n",
    "    max_duration=\"1ep\",  # train for more epochs to get better performance\n",
    "    optimizers=optimizer,\n",
    "    schedulers=[lr_scheduler],\n",
    "    device=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    precision=\"fp32\",\n",
    "    progress_bar=True,\n",
    "    loggers=[wandb_logger],\n",
    "    callbacks=[\n",
    "        SpeedMonitor(),\n",
    "        SampleCallback(\"To cook pasta, the first step is to\", tokenizer, \"100ba\"),\n",
    "    ],\n",
    "    # checkpointing\n",
    "    save_folder=save_folder,\n",
    "    save_filename=\"ep{epoch}-ba{batch}-rank{rank}.pt\",\n",
    "    save_interval=save_interval,\n",
    "    save_overwrite=True,\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Start training\n",
    "    trainer.fit()\n",
    "finally:\n",
    "    trainer.close()\n",
    "\n",
    "print(\"Saving model...\")\n",
    "# Save Hugging Face model\n",
    "config.save_pretrained(hf_save_folder)\n",
    "tokenizer.save_pretrained(hf_save_folder)\n",
    "composer_model.model.save_pretrained(hf_save_folder)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
