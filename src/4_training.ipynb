{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model with Composer\n",
    "\n",
    "Going to see how far I can get. Might have to leave it for fine-tuning and quantization, but it'll be nice for the base pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from composer.utils import reproducibility\n",
    "\n",
    "seed = 42\n",
    "reproducibility.seed_all(seed)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning about batch collation\n",
    "\n",
    "I imagined that we'd be creating a dataset where you get many samples per actual sample. Looks like this doesn't happen in the dataset itself; wondering if it happens in the dataloader or the model itself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tokenizer(\n",
    "    [\"Hello, world!\", \"12345 My name is John Kim and I like food\"],\n",
    "    truncation=True,\n",
    "    max_length=3,\n",
    "    return_length=True,\n",
    "    return_overflowing_tokens=True,  # without this, we only return the first sequence of max_length from each sample\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "test_dataset = Dataset.from_dict(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2832, 292, 11],\n",
       " 'token_type_ids': [0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1],\n",
       " 'length': 3,\n",
       " 'overflow_to_sample_mapping': 0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "# This collator just pads the sequences to the max length in the batch and vstacks them into a tensor\n",
    "# this forces the pad - with FastTokenizers it's better to pad before?\n",
    "# warning message\n",
    "# the problem is, for whatever reason, labels are shifted inside the model...?\n",
    "collate_fn = transformers.DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2832, 292, 11],\n",
       " [1461, 0],\n",
       " [167, 16, 17],\n",
       " [18, 19, 20],\n",
       " [1773, 709, 293],\n",
       " [2313, 395, 262],\n",
       " [260, 605, 866],\n",
       " [1687]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[\"input_ids\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs\n",
      " tensor([[2832,  292,   11],\n",
      "        [1461,    0, 8192],\n",
      "        [ 167,   16,   17],\n",
      "        [  18,   19,   20],\n",
      "        [1773,  709,  293],\n",
      "        [2313,  395,  262],\n",
      "        [ 260,  605,  866],\n",
      "        [1687, 8192, 8192]])\n",
      "Labels\n",
      " tensor([[2832,  292,   11],\n",
      "        [1461,    0, -100],\n",
      "        [ 167,   16,   17],\n",
      "        [  18,   19,   20],\n",
      "        [1773,  709,  293],\n",
      "        [2313,  395,  262],\n",
      "        [ 260,  605,  866],\n",
      "        [1687, -100, -100]])\n"
     ]
    }
   ],
   "source": [
    "collated = collate_fn([test_dataset[i] for i in range(len(test_dataset))])\n",
    "print(\"Input IDs\\n\", collated[\"input_ids\"])\n",
    "print(\"Labels\\n\", collated[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 292,   11, 2832],\n",
       "        [   0, -100, 1461],\n",
       "        [  16,   17,  167],\n",
       "        [  19,   20,   18],\n",
       "        [ 709,  293, 1773],\n",
       "        [ 395,  262, 2313],\n",
       "        [ 605,  866,  260],\n",
       "        [-100, -100, 1687]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.roll(collated[\"labels\"], -1, dims=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning about Cross Entropy Loss\n",
    "\n",
    "Implementation takes in tensors of (batch_size x vocab_size) and also a vector of (batch_size), where each element is the index of the correct word.\n",
    "\n",
    "Previously, I thought that it took in a tensor of (batch_size x vocab_size) and a tensor of (batch_size x vocab_size), where each element is the probability of the word. This is not the case.\n",
    "\n",
    "For whatever reason, during training, Hugging Face models expect the batch input_ids and the labels to be the same, even though the targets should really be shifted. The model internally shifts the labels during training...?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composer model definition\n",
    "\n",
    "Adapting my model to work with Composer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Either FairScale or torch distributed is not available, MixtureOfExperts will not be exposed. Please install them if you would like to use MoE\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "from composer.models.huggingface import HuggingFaceModel\n",
    "from composer.metrics.nlp import LanguageCrossEntropy\n",
    "import torch\n",
    "from torchmetrics import Metric\n",
    "from einops import rearrange\n",
    "from transformers import PreTrainedTokenizerFast, DataCollatorForLanguageModeling\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from model import WSConfig, WSModel\n",
    "\n",
    "\n",
    "class ComposerWSModel(HuggingFaceModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: WSConfig,\n",
    "        tokenizer: PreTrainedTokenizerFast,\n",
    "    ):\n",
    "        model = WSModel(config)\n",
    "\n",
    "        # this takes in pred and target logits\n",
    "        # should be batch_size x seq_len x vocab_size? probably\n",
    "        train_metrics: list[Metric] = [LanguageCrossEntropy()]\n",
    "\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            use_logits=True,\n",
    "            shift_labels=True,\n",
    "            metrics=train_metrics,\n",
    "        )\n",
    "\n",
    "        # Note: wanted to use flash-attn for fused CE, but there's an install error with rye\n",
    "        # Honestly should be pretty small relative to other things, not going to worry about it for now\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, batch: dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Mosaic's forward pass. Batch is a Mapping with keys possibly reflecting HuggingFace's forward function inputs.\n",
    "        Check GPT2 implementation for args; there isn't really a standard set.\n",
    "\n",
    "        Output needs to be an output dataclass from huggingface.\n",
    "        \"\"\"\n",
    "        return CausalLMOutputWithPast(\n",
    "            logits=self.model(batch[\"input_ids\"]),\n",
    "        )\n",
    "\n",
    "    def loss(self, outputs: CausalLMOutputWithPast, batch: dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Mosaic's loss function. Outputs is the output of the forward pass.\n",
    "        \"\"\"\n",
    "        # outputs is batch x seq_len x vocab_size\n",
    "        # labels is batch x seq_len\n",
    "        # need to reduce to (batch * seq_len) x vocab_size and (batch * seq_len)\n",
    "        output_logits = rearrange(\n",
    "            outputs.logits,\n",
    "            \"batch seq_len vocab_size -> (batch seq_len) vocab_size\",\n",
    "            vocab_size=self.config.vocab_size,\n",
    "        )\n",
    "        labels = batch[\"labels\"]\n",
    "        # shift labels left\n",
    "        labels = torch.roll(labels, -1, dims=1)\n",
    "        labels[:, -1] = -100  # don't predict the last token\n",
    "        # flatten\n",
    "        labels = rearrange(labels, \"batch seq_len -> (batch seq_len)\")\n",
    "\n",
    "        return self.loss_fn(output_logits, labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Model Training\n",
    "\n",
    "Let's do a tiny dataset with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import os\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "CACHE_DIR = \"/datadrive/hf_cache\"\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"tokenizer\")\n",
    "config = WSConfig(\n",
    "    d_model=64,\n",
    "    n_heads=4,\n",
    "    n_layers=2,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikihow (/datadrive/hf_cache/wikihow/all-data_dir=%2Fdatadrive%2Fhf_cache/1.2.0/5343fc81d685acaa086c9cc19eb8706206cd1f8b315792b04c1d7b92091c305e)\n",
      "Loading cached shuffled indices for dataset at /datadrive/hf_cache/wikihow/all-data_dir=%2Fdatadrive%2Fhf_cache/1.2.0/5343fc81d685acaa086c9cc19eb8706206cd1f8b315792b04c1d7b92091c305e/cache-ca61b0a7a4447ccd.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4cbe9c046ba490ca5335b068a3b07be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/157252 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build dataloader\n",
    "import torch.utils.data\n",
    "\n",
    "wikihow_data: datasets.Dataset = datasets.load_dataset(\n",
    "    \"wikihow\",\n",
    "    name=\"all\",\n",
    "    data_dir=CACHE_DIR,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    use_auth_token=HF_TOKEN,\n",
    "    split=\"train\",\n",
    "    # streaming=True,\n",
    ").shuffle(\n",
    "    seed=seed\n",
    ")  # type: ignore\n",
    "\n",
    "text_column_name = \"text\"\n",
    "\n",
    "\n",
    "def tokenize_function(examples: dict[str, Any]):\n",
    "    examples[text_column_name] = [\n",
    "        line\n",
    "        for line in examples[text_column_name]\n",
    "        if len(line) > 0 and not line.isspace()\n",
    "    ]\n",
    "    return tokenizer(\n",
    "        examples[text_column_name],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized_train = wikihow_data.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=wikihow_data.column_names,  # collate_fn doesn't like other columns\n",
    "    load_from_cache_file=False,\n",
    ")\n",
    "\n",
    "collate_fn = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    tokenized_train, batch_size=64, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from composer.optim import DecoupledAdamW, LinearWithWarmupScheduler\n",
    "\n",
    "composer_model = ComposerWSModel(config=config, tokenizer=tokenizer)\n",
    "optimizer = DecoupledAdamW(\n",
    "    composer_model.model.parameters(),\n",
    "    lr=1.0e-4,\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1.0e-06,\n",
    "    weight_decay=1.0e-5,\n",
    ")\n",
    "lr_scheduler = LinearWithWarmupScheduler(t_warmup=\"250ba\", alpha_f=0.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Config:\n",
      "node_name: unknown because NODENAME environment variable not set\n",
      "num_gpus_per_node: 1\n",
      "num_nodes: 1\n",
      "rank_zero_seed: 300766860\n",
      "\n",
      "******************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42518a75f7f4444966ada1fdf307eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   0:    0%|| 0/2458 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 22\u001b[0m\n\u001b[1;32m      5\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      6\u001b[0m     model\u001b[39m=\u001b[39mcomposer_model,  \u001b[39m# This is the model from the HuggingFaceModel wrapper class.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     train_dataloader\u001b[39m=\u001b[39mtrain_dataloader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     save_interval\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m10ba\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m trainer\u001b[39m.\u001b[39;49mfit()\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/composer/trainer/trainer.py:1804\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, train_dataloader, train_dataloader_label, train_subset_num_batches, spin_dataloaders, duration, reset_time, schedulers, scale_schedule_ratio, step_schedulers_every_batch, eval_dataloader, eval_subset_num_batches, eval_interval, device_train_microbatch_size, precision)\u001b[0m\n\u001b[1;32m   1801\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mscaler \u001b[39m=\u001b[39m ClosureGradScaler() \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_closures() \u001b[39melse\u001b[39;00m GradScaler()\n\u001b[1;32m   1803\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfirst_batch_complete \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1804\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_loop()\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/composer/trainer/trainer.py:1979\u001b[0m, in \u001b[0;36mTrainer._train_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1976\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mlog_metrics({\u001b[39m'\u001b[39m\u001b[39mtime/token\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mtimestamp\u001b[39m.\u001b[39mtoken\u001b[39m.\u001b[39mvalue})\n\u001b[1;32m   1977\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mlog_metrics({\u001b[39m'\u001b[39m\u001b[39mtime/token_in_epoch\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mtimestamp\u001b[39m.\u001b[39mtoken_in_epoch\u001b[39m.\u001b[39mvalue})\n\u001b[0;32m-> 1979\u001b[0m total_loss_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_batch(use_grad_scaling)\n\u001b[1;32m   1981\u001b[0m \u001b[39mif\u001b[39;00m use_grad_scaling:\n\u001b[1;32m   1982\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mupdate()\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/composer/trainer/trainer.py:2163\u001b[0m, in \u001b[0;36mTrainer._train_batch\u001b[0;34m(self, use_grad_scaling)\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mstep(optimizer,\n\u001b[1;32m   2160\u001b[0m                                    closure\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m loss_dict\u001b[39m=\u001b[39mtotal_loss_dict, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m   2161\u001b[0m                                    _train_microbatches(microbatches, loss_dict, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs))\n\u001b[1;32m   2162\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2163\u001b[0m             optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m loss_dict\u001b[39m=\u001b[39;49mtotal_loss_dict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_microbatches(\n\u001b[1;32m   2164\u001b[0m                 microbatches, loss_dict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39;49mitem())\n\u001b[1;32m   2165\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2166\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_microbatches(microbatches, total_loss_dict)\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     68\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/composer/optim/decoupled_weight_decay.py:288\u001b[0m, in \u001b[0;36mDecoupledAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[0;32m--> 288\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    290\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[1;32m    291\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/composer/trainer/trainer.py:2163\u001b[0m, in \u001b[0;36mTrainer._train_batch.<locals>.<lambda>\u001b[0;34m(loss_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mstep(optimizer,\n\u001b[1;32m   2160\u001b[0m                                    closure\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m loss_dict\u001b[39m=\u001b[39mtotal_loss_dict, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m   2161\u001b[0m                                    _train_microbatches(microbatches, loss_dict, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs))\n\u001b[1;32m   2162\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2163\u001b[0m             optimizer\u001b[39m.\u001b[39mstep(closure\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m loss_dict\u001b[39m=\u001b[39mtotal_loss_dict, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_microbatches(\n\u001b[1;32m   2164\u001b[0m                 microbatches, loss_dict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39mitem())\n\u001b[1;32m   2165\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2166\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_microbatches(microbatches, total_loss_dict)\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/composer/trainer/trainer.py:2266\u001b[0m, in \u001b[0;36mTrainer._train_microbatches\u001b[0;34m(self, microbatches, total_loss_dict, ddp_sync)\u001b[0m\n\u001b[1;32m   2264\u001b[0m \u001b[39mfor\u001b[39;00m microbatch_idx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mbatch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(microbatches):\n\u001b[1;32m   2265\u001b[0m     is_final_microbatch \u001b[39m=\u001b[39m microbatch_idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(microbatches)\n\u001b[0;32m-> 2266\u001b[0m     microbatch_loss_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_microbatch(use_grad_scaling, current_batch_size, is_final_microbatch)\n\u001b[1;32m   2268\u001b[0m     \u001b[39m# Aggregate each loss in microbatch_loss_dict into total_loss_dict\u001b[39;00m\n\u001b[1;32m   2269\u001b[0m     \u001b[39mfor\u001b[39;00m k, microbatch_loss \u001b[39min\u001b[39;00m microbatch_loss_dict\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/composer/trainer/trainer.py:2393\u001b[0m, in \u001b[0;36mTrainer._train_microbatch\u001b[0;34m(self, use_grad_scaling, current_batch_size, is_final_microbatch)\u001b[0m\n\u001b[1;32m   2390\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2391\u001b[0m     \u001b[39m# Scale loss based on the number of samples in the microbatch to maintain gradient numerics\u001b[39;00m\n\u001b[1;32m   2392\u001b[0m     microbatch_loss\u001b[39m.\u001b[39mmul_(microbatch_num_samples \u001b[39m/\u001b[39m current_batch_size)\n\u001b[0;32m-> 2393\u001b[0m     microbatch_loss\u001b[39m.\u001b[39;49mbackward(create_graph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backwards_create_graph)\n\u001b[1;32m   2395\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine\u001b[39m.\u001b[39mrun_event(Event\u001b[39m.\u001b[39mAFTER_BACKWARD)\n\u001b[1;32m   2397\u001b[0m \u001b[39m# Use microbatch outputs to update training metrics\u001b[39;00m\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from composer import Trainer\n",
    "\n",
    "# Create Trainer Object\n",
    "trainer = Trainer(\n",
    "    model=composer_model,  # This is the model from the HuggingFaceModel wrapper class.\n",
    "    train_dataloader=train_dataloader,\n",
    "    # eval_dataloader=eval_dataloader,\n",
    "    max_duration=\"1ep\",  # train for more epochs to get better performance\n",
    "    optimizers=optimizer,\n",
    "    schedulers=[lr_scheduler],\n",
    "    device=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    # train_subset_num_batches=100, # uncomment this line to only run part of training, which will be faster\n",
    "    precision=\"fp32\",\n",
    "    progress_bar=True,\n",
    "    # checkpointing\n",
    "    save_folder=\"checkpoints/pretraining/\",\n",
    "    save_filename=\"ep{epoch}-ba{batch}-rank{rank}.pt\",\n",
    "    save_interval=\"500ba\",\n",
    ")\n",
    "# Start training\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composer_model.model.save_pretrained(\"model/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
