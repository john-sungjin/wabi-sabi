{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model with Composer\n",
    "\n",
    "Going to see how far I can get. Might have to leave it for fine-tuning and quantization, but it'll be nice for the base pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from composer.utils import reproducibility\n",
    "\n",
    "seed = 42\n",
    "reproducibility.seed_all(seed)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning about batch collation\n",
    "\n",
    "I imagined that we'd be creating a dataset where you get many samples per actual sample. Looks like this doesn't happen in the dataset itself; wondering if it happens in the dataloader or the model itself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tokenizer(\n",
    "    [\"Hello, world!\", \"12345 My name is John Kim and I like food\"],\n",
    "    truncation=True,\n",
    "    max_length=3,\n",
    "    return_length=True,\n",
    "    return_overflowing_tokens=True,  # without this, we only return the first sequence of max_length from each sample\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "test_dataset = Dataset.from_dict(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2832, 292, 11],\n",
       " 'token_type_ids': [0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1],\n",
       " 'length': 3,\n",
       " 'overflow_to_sample_mapping': 0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "# This collator just pads the sequences to the max length in the batch and vstacks them into a tensor\n",
    "# this forces the pad - with FastTokenizers it's better to pad before?\n",
    "# warning message\n",
    "# the problem is, for whatever reason, labels are shifted inside the model...?\n",
    "collate_fn = transformers.DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2832, 292, 11],\n",
       " [1461, 0],\n",
       " [167, 16, 17],\n",
       " [18, 19, 20],\n",
       " [1773, 709, 293],\n",
       " [2313, 395, 262],\n",
       " [260, 605, 866],\n",
       " [1687]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[\"input_ids\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs\n",
      " tensor([[2832,  292,   11],\n",
      "        [1461,    0, 8192],\n",
      "        [ 167,   16,   17],\n",
      "        [  18,   19,   20],\n",
      "        [1773,  709,  293],\n",
      "        [2313,  395,  262],\n",
      "        [ 260,  605,  866],\n",
      "        [1687, 8192, 8192]])\n",
      "Labels\n",
      " tensor([[2832,  292,   11],\n",
      "        [1461,    0, -100],\n",
      "        [ 167,   16,   17],\n",
      "        [  18,   19,   20],\n",
      "        [1773,  709,  293],\n",
      "        [2313,  395,  262],\n",
      "        [ 260,  605,  866],\n",
      "        [1687, -100, -100]])\n"
     ]
    }
   ],
   "source": [
    "collated = collate_fn([test_dataset[i] for i in range(len(test_dataset))])\n",
    "print(\"Input IDs\\n\", collated[\"input_ids\"])\n",
    "print(\"Labels\\n\", collated[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 292,   11, 2832],\n",
       "        [   0, -100, 1461],\n",
       "        [  16,   17,  167],\n",
       "        [  19,   20,   18],\n",
       "        [ 709,  293, 1773],\n",
       "        [ 395,  262, 2313],\n",
       "        [ 605,  866,  260],\n",
       "        [-100, -100, 1687]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.roll(collated[\"labels\"], -1, dims=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning about Cross Entropy Loss\n",
    "\n",
    "Implementation takes in tensors of (batch_size x vocab_size) and also a vector of (batch_size), where each element is the index of the correct word.\n",
    "\n",
    "Previously, I thought that it took in a tensor of (batch_size x vocab_size) and a tensor of (batch_size x vocab_size), where each element is the probability of the word. This is not the case.\n",
    "\n",
    "For whatever reason, during training, Hugging Face models expect the batch input_ids and the labels to be the same, even though the targets should really be shifted. The model internally shifts the labels during training...?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composer model definition\n",
    "\n",
    "Adapting my model to work with Composer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from composer.models.huggingface import HuggingFaceModel\n",
    "from composer.metrics.nlp import LanguageCrossEntropy\n",
    "import torch\n",
    "from torchmetrics import Metric\n",
    "from einops import rearrange\n",
    "from transformers import PreTrainedTokenizerFast, DataCollatorForLanguageModeling\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from model import WSConfig, WSModel\n",
    "\n",
    "\n",
    "class ComposerWSModel(HuggingFaceModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: WSConfig,\n",
    "        tokenizer: PreTrainedTokenizerFast,\n",
    "    ):\n",
    "        model = WSModel(config)\n",
    "\n",
    "        # this takes in pred and target logits\n",
    "        # should be batch_size x seq_len x vocab_size? probably\n",
    "        train_metrics: list[Metric] = [LanguageCrossEntropy()]\n",
    "\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            use_logits=True,\n",
    "            shift_labels=True,\n",
    "            metrics=train_metrics,\n",
    "        )\n",
    "\n",
    "        # Note: wanted to use flash-attn for fused CE, but there's an install error with rye\n",
    "        # Honestly should be pretty small relative to other things, not going to worry about it for now\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, batch: dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Mosaic's forward pass. Batch is a Mapping with keys possibly reflecting HuggingFace's forward function inputs.\n",
    "        Check GPT2 implementation for args; there isn't really a standard set.\n",
    "\n",
    "        Output needs to be an output dataclass from huggingface.\n",
    "        \"\"\"\n",
    "        return self.model(batch[\"input_ids\"])\n",
    "\n",
    "    def loss(self, outputs: CausalLMOutputWithPast, batch: dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Mosaic's loss function. Outputs is the output of the forward pass.\n",
    "        \"\"\"\n",
    "        # outputs is batch x seq_len x vocab_size\n",
    "        # labels is batch x seq_len\n",
    "        # need to reduce to (batch * seq_len) x vocab_size and (batch * seq_len)\n",
    "        output_logits = rearrange(\n",
    "            outputs.logits,\n",
    "            \"batch seq_len vocab_size -> (batch seq_len) vocab_size\",\n",
    "            vocab_size=self.config.vocab_size,\n",
    "        )\n",
    "        labels = batch[\"labels\"]\n",
    "        # shift labels left\n",
    "        labels = torch.roll(labels, -1, dims=1)\n",
    "        labels[:, -1] = -100  # don't predict the last token\n",
    "        # flatten\n",
    "        labels = rearrange(labels, \"batch seq_len -> (batch seq_len)\")\n",
    "\n",
    "        return self.loss_fn(output_logits, labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Model Training\n",
    "\n",
    "Let's do a tiny dataset with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import os\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "CACHE_DIR = \"/datadrive/hf_cache\"\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"tokenizer\")\n",
    "config = WSConfig(\n",
    "    d_model=64,\n",
    "    n_heads=4,\n",
    "    n_layers=2,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikihow (/datadrive/hf_cache/wikihow/all-data_dir=%2Fdatadrive%2Fhf_cache/1.2.0/5343fc81d685acaa086c9cc19eb8706206cd1f8b315792b04c1d7b92091c305e)\n",
      "Loading cached shuffled indices for dataset at /datadrive/hf_cache/wikihow/all-data_dir=%2Fdatadrive%2Fhf_cache/1.2.0/5343fc81d685acaa086c9cc19eb8706206cd1f8b315792b04c1d7b92091c305e/cache-ca61b0a7a4447ccd.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4cbe9c046ba490ca5335b068a3b07be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/157252 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build dataloader\n",
    "import torch.utils.data\n",
    "\n",
    "wikihow_data: datasets.Dataset = datasets.load_dataset(\n",
    "    \"wikihow\",\n",
    "    name=\"all\",\n",
    "    data_dir=CACHE_DIR,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    use_auth_token=HF_TOKEN,\n",
    "    split=\"train\",\n",
    "    # streaming=True,\n",
    ").shuffle(\n",
    "    seed=seed\n",
    ")  # type: ignore\n",
    "\n",
    "text_column_name = \"text\"\n",
    "\n",
    "\n",
    "def tokenize_function(examples: dict[str, Any]):\n",
    "    examples[text_column_name] = [\n",
    "        line\n",
    "        for line in examples[text_column_name]\n",
    "        if len(line) > 0 and not line.isspace()\n",
    "    ]\n",
    "    return tokenizer(\n",
    "        examples[text_column_name],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized_train = wikihow_data.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=wikihow_data.column_names,  # collate_fn doesn't like other columns\n",
    "    load_from_cache_file=False,\n",
    ")\n",
    "\n",
    "collate_fn = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    tokenized_train, batch_size=64, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from composer.optim import DecoupledAdamW, LinearWithWarmupScheduler\n",
    "\n",
    "composer_model = ComposerWSModel(config=config, tokenizer=tokenizer)\n",
    "optimizer = DecoupledAdamW(\n",
    "    composer_model.model.parameters(),\n",
    "    lr=1.0e-4,\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1.0e-06,\n",
    "    weight_decay=1.0e-5,\n",
    ")\n",
    "lr_scheduler = LinearWithWarmupScheduler(t_warmup=\"250ba\", alpha_f=0.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Config:\n",
      "node_name: unknown because NODENAME environment variable not set\n",
      "num_gpus_per_node: 1\n",
      "num_nodes: 1\n",
      "rank_zero_seed: 3853855035\n",
      "\n",
      "******************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2dd69fceb941cba90973a79fe27e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   0:    0%|| 0/2458 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/composer/core/data_spec.py:35: UserWarning: Cannot split tensor of length 4 into batches of size 64. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.\n",
      "  warnings.warn(f'Cannot split tensor of length {len(t)} into batches of size {microbatch_size}. '\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from composer import Trainer\n",
    "\n",
    "# Create Trainer Object\n",
    "trainer = Trainer(\n",
    "    model=composer_model,  # This is the model from the HuggingFaceModel wrapper class.\n",
    "    train_dataloader=train_dataloader,\n",
    "    # eval_dataloader=eval_dataloader,\n",
    "    max_duration=\"1ep\",  # train for more epochs to get better performance\n",
    "    optimizers=optimizer,\n",
    "    schedulers=[lr_scheduler],\n",
    "    device=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    # train_subset_num_batches=100, # uncomment this line to only run part of training, which will be faster\n",
    "    precision=\"fp32\",\n",
    "    progress_bar=True,\n",
    "    # checkpointing\n",
    "    save_folder=\"checkpoints/pretraining/\",\n",
    "    save_filename=\"ep{epoch}-ba{batch}-rank{rank}.pt\",\n",
    "    save_interval=\"500ba\",\n",
    "    save_overwrite=True,\n",
    ")\n",
    "# Start training\n",
    "trainer.fit()\n",
    "composer_model.model.save_pretrained(\"model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# going to see if I can save my model to Hugging Face\n",
    "\n",
    "hf_model_dir = \"huggingface_model/\"\n",
    "\n",
    "# save config\n",
    "config.save_pretrained(hf_model_dir)\n",
    "\n",
    "# save tokenizer\n",
    "tokenizer.save_pretrained(hf_model_dir)\n",
    "\n",
    "# save model\n",
    "composer_model.model.register_for_auto_class\n",
    "composer_model.model.save_pretrained(hf_model_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing saved checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt IDs: tensor([[ 260,   47,   12, 1774,  710,  294]])\n",
      "Output IDs: tensor([[ 260,   47,   12, 1774,  710,  294,  208, 1181,   12,  258,  404,  318,\n",
      "          208, 1181,   12,  258,  404,  318,  208, 1181,   12,  258,  404,  318,\n",
      "          208, 1181]])\n",
      "Output Text:  hi, my name is a good, you can be a good, you can be a good, you can be a good\n"
     ]
    }
   ],
   "source": [
    "test_config = WSConfig.from_pretrained(hf_model_dir)\n",
    "test_model = WSModel.from_pretrained(hf_model_dir, config=test_config)\n",
    "test_tokenizer = PreTrainedTokenizerFast.from_pretrained(hf_model_dir)\n",
    "\n",
    "prompt = \"Hi, my name is\"\n",
    "prompt_ids = test_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "print(\"Prompt IDs:\", prompt_ids)\n",
    "\n",
    "\n",
    "output_ids = test_model.generate(prompt_ids, max_new_tokens=20)\n",
    "print(\"Output IDs:\", output_ids)\n",
    "\n",
    "output_text = test_tokenizer.decode(output_ids[0])\n",
    "print(\"Output Text:\", output_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
