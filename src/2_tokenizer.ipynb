{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer Training\n",
    "\n",
    "I'm trying to train a model that knows one language (English) and one programming language (Python). I want it to be able to reason - I'm thinking I'll need to find some reasoning datasets or something to train on...\n",
    "\n",
    "For now, I'll use Wikipedia and Python to train my tokenizers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some papers\n",
    "\n",
    "Model behavior at reduced scale: https://arxiv.org/abs/2305.17266\n",
    "\n",
    "On width vs depth (linked in Chinchilla paper): https://arxiv.org/pdf/2006.12467.pdf\n",
    "\n",
    "A vocabulary bottleneck: https://arxiv.org/pdf/2006.12467.pdf\n",
    "\n",
    "Transformer architectures vary in depth/width ratios, but in language they're pretty consistent\n",
    "\n",
    "\n",
    "Impact of tokenization on language models: https://arxiv.org/pdf/2204.08832.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from tokenizers import (\n",
    "    Tokenizer,\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    ")\n",
    "from transformers import (\n",
    "    PreTrainedTokenizerFast,\n",
    "    LlamaTokenizer,\n",
    "    AutoTokenizer,\n",
    "    GPTNeoXTokenizerFast,\n",
    "    LlamaTokenizerFast,\n",
    ")\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from composer.utils import reproducibility\n",
    "import psutil\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "CACHE_DIR = \"/datadrive/hf_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/datadrive/hf_cache\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning from the Llama Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    \"openlm-research/open_llama_3b\", cache_dir=CACHE_DIR\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 31822, 31853, 31855, 31852, 31853, 31878]\n",
      "['<s>', '▁', '1', '2', '0', '1', '3']\n",
      "[1, 27701]\n",
      "[1, 27701, 31843]\n"
     ]
    }
   ],
   "source": [
    "tokens = llama_tokenizer.encode(\"12013\")\n",
    "print(tokens)\n",
    "print(llama_tokenizer.convert_ids_to_tokens(tokens))\n",
    "print(llama_tokenizer.encode(\"hello\"))\n",
    "print(llama_tokenizer.encode(\"hello.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_tokenizer.special_tokens_map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training my tokenizer\n",
    "\n",
    "Want to take all of the datasets I have, merge them, and shuffle them for the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    print(f\"Memory used: {mem_info.rss / (1024**2)} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "reproducibility.seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used: 528.5703125 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikipedia (/datadrive/hf_cache/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n",
      "Loading cached shuffled indices for dataset at /datadrive/hf_cache/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559/cache-9d1514c538bda335.arrow\n",
      "Found cached dataset parquet (/datadrive/hf_cache/bigcode___parquet/bigcode--the-stack-dedup-d5df9d0729d2a04a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Loading cached shuffled indices for dataset at /datadrive/hf_cache/bigcode___parquet/bigcode--the-stack-dedup-d5df9d0729d2a04a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-f3d5cb76f5b9c67e.arrow\n",
      "Found cached dataset wikihow (/datadrive/hf_cache/wikihow/all-data_dir=%2Fdatadrive%2Fhf_cache/1.2.0/5343fc81d685acaa086c9cc19eb8706206cd1f8b315792b04c1d7b92091c305e)\n",
      "Loading cached shuffled indices for dataset at /datadrive/hf_cache/wikihow/all-data_dir=%2Fdatadrive%2Fhf_cache/1.2.0/5343fc81d685acaa086c9cc19eb8706206cd1f8b315792b04c1d7b92091c305e/cache-ca61b0a7a4447ccd.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used: 1897.77734375 MB\n"
     ]
    }
   ],
   "source": [
    "# Load all datasets\n",
    "# streaming because https://huggingface.co/docs/datasets/v2.13.1/en/about_mapstyle_vs_iterable\n",
    "\n",
    "print_memory_usage()\n",
    "wikipedia_dataset: datasets.IterableDataset = load_dataset(\n",
    "    \"wikipedia\",\n",
    "    name=\"20220301.en\",\n",
    "    cache_dir=CACHE_DIR,\n",
    "    use_auth_token=HF_TOKEN,\n",
    "    split=\"train\",\n",
    "    # streaming=True,\n",
    ").shuffle(\n",
    "    seed=seed\n",
    ")  # type: ignore\n",
    "python_stack_dataset: datasets.IterableDataset = (\n",
    "    load_dataset(\n",
    "        \"bigcode/the-stack-dedup\",\n",
    "        cache_dir=CACHE_DIR,\n",
    "        data_dir=\"data/python\",\n",
    "        use_auth_token=HF_TOKEN,\n",
    "        split=\"train\",\n",
    "        # streaming=True,\n",
    "    )\n",
    "    .shuffle(seed=seed)\n",
    "    .rename_column(\"content\", \"text\")\n",
    ")  # type: ignore\n",
    "wikihow_data: datasets.IterableDataset = load_dataset(\n",
    "    \"wikihow\",\n",
    "    name=\"all\",\n",
    "    data_dir=CACHE_DIR,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    use_auth_token=HF_TOKEN,\n",
    "    split=\"train\",\n",
    "    # streaming=True,\n",
    ").shuffle(\n",
    "    seed=seed\n",
    ")  # type: ignore\n",
    "print_memory_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Sizes (in GB)\n",
      "Wikipedia: 18.88304591178894\n",
      "Python: 66.9516989979893\n",
      "Wikihow: 0.4779902445152402\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Sizes (in GB)\")\n",
    "print(\"Wikipedia:\", wikipedia_dataset.info.splits[\"train\"].num_bytes / (1024**3))\n",
    "print(\"Python:\", python_stack_dataset.info.splits[\"train\"].num_bytes / (1024**3))\n",
    "print(\"Wikihow:\", wikihow_data.info.splits[\"train\"].num_bytes / (1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tokenizer\n",
    "# Want to control data mixture\n",
    "# Wikipedia 30%,Python 40%, Wikihow 30%? Seems reasonable\n",
    "\n",
    "dataset = datasets.interleave_datasets(\n",
    "    [wikipedia_dataset, python_stack_dataset, wikihow_data],\n",
    "    probabilities=[0.3, 0.4, 0.3],\n",
    "    seed=seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "def batch_generator(\n",
    "    dataset: datasets.Dataset, batch_size: int = 1000, converter: Callable = lambda x: x\n",
    "):\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield converter(dataset[i : i + batch_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(\n",
    "    models.BPE(\n",
    "        vocab=None,\n",
    "        merges=None,\n",
    "        unk_token=None,\n",
    "        dropout=None,\n",
    "        fuse_unk=False,\n",
    "    )\n",
    ")\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.NFKC(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
    ")  # type: ignore\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n",
    "    [\n",
    "        pre_tokenizers.ByteLevel(add_prefix_space=True),\n",
    "        pre_tokenizers.Digits(individual_digits=True),\n",
    "    ]\n",
    ")  # type: ignore\n",
    "tokenizer.post_processor = processors.ByteLevel()  # type: ignore\n",
    "tokenizer.decoder = decoders.Sequence(\n",
    "    [\n",
    "        decoders.ByteLevel(),\n",
    "    ]\n",
    ")  # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized: hello world! my name is john\n",
      "Pre-tokenized: [('hello', (0, 5)), ('Ġworld', (5, 11)), ('!', (11, 12)), ('Ġmy', (12, 15)), ('Ġname', (15, 20)), ('Ġis', (20, 23)), ('Ġjohn', (23, 28))]\n",
      "Encoded: []\n"
     ]
    }
   ],
   "source": [
    "test_str = \"Hello world! My name is John\"\n",
    "normalized = tokenizer.normalizer.normalize_str(test_str)\n",
    "print(\"Normalized:\", normalized)\n",
    "pre_tokenized = tokenizer.pre_tokenizer.pre_tokenize_str(normalized)\n",
    "print(\"Pre-tokenized:\", pre_tokenized)\n",
    "print(\"Encoded:\", tokenizer.encode(test_str).tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(\n",
    "    iterator=dataset.iter(1000),\n",
    "    trainer=trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1713: FutureWarning: Calling PreTrainedTokenizerFast.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁ h e l l o ▁ w o r l d'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer(\"Hello World\")[\"input_ids\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
