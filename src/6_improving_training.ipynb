{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving training\n",
    "\n",
    "Now that basic training works, there's a few things I need to add in:\n",
    "\n",
    "- Attention masking: I need to mask out the attention for the padding tokens. We already should get the attention mask from the data loader, so this should be pretty straightforward\n",
    "- Metrics: I'd like to be able to track loss, FLOPs, memory usage, etc.\n",
    "- Optimization: what training optimizations can I make? E.g. mixed precision training, gradient accumulation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from composer.utils import reproducibility\n",
    "\n",
    "seed = 42\n",
    "reproducibility.seed_all(seed)\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "CACHE_DIR = \"/datadrive/hf_cache\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating Attention Masking\n",
    "\n",
    "I'm realizing the attention masking doesn't really matter?\n",
    "Since it's causal, having padding tokens at the end doesn't affect preceding logits. It won't affect the loss either, since we set those labels to -100.\n",
    "\n",
    "I think it's not worth adding into the code; the is_causal flag is enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikihow (/datadrive/hf_cache/wikihow/all-data_dir=%2Fdatadrive%2Fhf_cache/1.2.0/5343fc81d685acaa086c9cc19eb8706206cd1f8b315792b04c1d7b92091c305e)\n",
      "Loading cached shuffled indices for dataset at /datadrive/hf_cache/wikihow/all-data_dir=%2Fdatadrive%2Fhf_cache/1.2.0/5343fc81d685acaa086c9cc19eb8706206cd1f8b315792b04c1d7b92091c305e/cache-ca61b0a7a4447ccd.arrow\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "import datasets\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "wikihow_data: datasets.Dataset = datasets.load_dataset(\n",
    "    \"wikihow\",\n",
    "    name=\"all\",\n",
    "    data_dir=CACHE_DIR,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    use_auth_token=HF_TOKEN,\n",
    "    split=\"train\",\n",
    "    # streaming=True,\n",
    ").shuffle(\n",
    "    seed=seed\n",
    ")  # type: ignore\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids [220, 2160, 2718, 3914, 254, 6906, 6060, 413, 232, 687, 0, 0, 0, 0, 0]\n",
      "attention_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "seq_len = 6\n",
    "sample = wikihow_data[0][\"text\"][:50]\n",
    "tokenized_sample = tokenizer(\n",
    "    sample,\n",
    "    padding=\"max_length\",\n",
    "    max_length=seq_len,\n",
    ")\n",
    "print(\"input_ids\", tokenized_sample[\"input_ids\"])\n",
    "print(\"attention_mask\", tokenized_sample[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True, False, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "sample_qk = torch.randn(seq_len, seq_len)\n",
    "attn_mask = torch.ones(seq_len, seq_len, dtype=torch.bool).tril(diagonal=0)\n",
    "print(attn_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import WSModel, WSConfig\n",
    "\n",
    "model = WSModel(WSConfig())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "Just gonna paste my whole training script here and mess with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikihow (/datadrive/hf_cache/wikihow/all-data_dir=%2Fdatadrive%2Fhf_cache/1.2.0/5343fc81d685acaa086c9cc19eb8706206cd1f8b315792b04c1d7b92091c305e)\n",
      "Loading cached shuffled indices for dataset at /datadrive/hf_cache/wikihow/all-data_dir=%2Fdatadrive%2Fhf_cache/1.2.0/5343fc81d685acaa086c9cc19eb8706206cd1f8b315792b04c1d7b92091c305e/cache-ca61b0a7a4447ccd.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1b545885224bc9876c5dd9f4039867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/157252 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from typing import Any\n",
    "\n",
    "import datasets\n",
    "import torch.utils.data\n",
    "from composer import Trainer\n",
    "from composer.optim import DecoupledAdamW, LinearWithWarmupScheduler\n",
    "from composer.utils import reproducibility\n",
    "from model import ComposerWSModel, WSConfig\n",
    "from transformers import DataCollatorForLanguageModeling, PreTrainedTokenizerFast\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "CACHE_DIR = \"/datadrive/hf_cache\"\n",
    "\n",
    "###### CONFIG ######\n",
    "model_params = {\n",
    "    \"d_model\": 64,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_layers\": 2,\n",
    "    \"vocab_size\": 8192,\n",
    "}\n",
    "\n",
    "seed = 42\n",
    "optim = {\n",
    "    \"lr\": 1e-4,\n",
    "    \"betas\": (0.9, 0.98),\n",
    "    \"eps\": 1.0e-06,\n",
    "    \"weight_decay\": 1.0e-5,\n",
    "}\n",
    "learning_rate = {\"t_warmup\": \"250ba\", \"alpha_f\": 0.02}\n",
    "precision = \"fp32\"\n",
    "\n",
    "save_folder = \"checkpoints/pretraining/\"\n",
    "save_interval = \"500ba\"\n",
    "hf_save_folder = \"huggingface_model/\"\n",
    "\n",
    "tokenizer_dir = \"tokenizer/\"\n",
    "###### END CONFIG ######\n",
    "\n",
    "\n",
    "reproducibility.seed_all(seed)\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_dir)\n",
    "config = WSConfig(**model_params)\n",
    "\n",
    "text_column_name = \"text\"\n",
    "\n",
    "\n",
    "def tokenize_function(examples: dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Tokenize dataset examples.\n",
    "    \"\"\"\n",
    "    examples[text_column_name] = [\n",
    "        line\n",
    "        for line in examples[text_column_name]\n",
    "        if len(line) > 0 and not line.isspace()\n",
    "    ]\n",
    "    return tokenizer(\n",
    "        examples[text_column_name],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "wikihow_data: datasets.Dataset = datasets.load_dataset(\n",
    "    \"wikihow\",\n",
    "    name=\"all\",\n",
    "    data_dir=CACHE_DIR,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    use_auth_token=HF_TOKEN,\n",
    "    split=\"train\",\n",
    "    # streaming=True,\n",
    ").shuffle(\n",
    "    seed=seed\n",
    ")  # type: ignore\n",
    "\n",
    "tokenized_train = wikihow_data.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=wikihow_data.column_names,  # collate_fn doesn't like other columns\n",
    "    load_from_cache_file=False,\n",
    ")\n",
    "\n",
    "collate_fn = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    tokenized_train, batch_size=64, collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "composer_model = ComposerWSModel(config=config, tokenizer=tokenizer)\n",
    "optimizer = DecoupledAdamW(\n",
    "    composer_model.model.parameters(),\n",
    "    # lr=1.0e-4,\n",
    "    # betas=(0.9, 0.98),\n",
    "    # eps=1.0e-06,\n",
    "    # weight_decay=1.0e-5,\n",
    "    **optim,\n",
    ")\n",
    "lr_scheduler = LinearWithWarmupScheduler(**learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from composer.loggers import WandBLogger\n",
    "from composer.callbacks import SpeedMonitor\n",
    "\n",
    "wandb_logger = WandBLogger(project=\"wabisabi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/datadrive/wabi-sabi/src/wandb/run-20230627_050341-z4zgh8xz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/john-sungjin/wabisabi/runs/z4zgh8xz' target=\"_blank\">1687841559-adaptable-cricket</a></strong> to <a href='https://wandb.ai/john-sungjin/wabisabi' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/john-sungjin/wabisabi' target=\"_blank\">https://wandb.ai/john-sungjin/wabisabi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/john-sungjin/wabisabi/runs/z4zgh8xz' target=\"_blank\">https://wandb.ai/john-sungjin/wabisabi/runs/z4zgh8xz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Config:\n",
      "node_name: unknown because NODENAME environment variable not set\n",
      "num_gpus_per_node: 1\n",
      "num_nodes: 1\n",
      "rank_zero_seed: 1046410796\n",
      "\n",
      "******************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6215152d30742c59d7429c01e6f8885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   0:    0%|| 0/2458 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/composer/core/data_spec.py:35: UserWarning: Cannot split tensor of length 4 into batches of size 64. As it is smaller, no splitting will be done. This may happen on the last batch of a dataset if it is a smaller size than the microbatch size.\n",
      "  warnings.warn(f'Cannot split tensor of length {len(t)} into batches of size {microbatch_size}. '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss/train/total</td><td>██▇▆▅▄▄▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/train/LanguageCrossEntropy</td><td>██▇▆▅▄▄▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>throughput/batches_per_sec</td><td>█▇▆▆▆▆▅▄▄▅▅▅▆▄▄▅▄▄▄▃▃▃▄▄▃▄▄▅▄▅▄▂▁▂▃▂▁▁▁▁</td></tr><tr><td>throughput/device/batches_per_sec</td><td>█▇▆▆▆▆▅▄▄▅▅▅▆▄▄▅▄▄▄▃▃▃▄▄▃▄▄▅▄▅▄▂▁▂▃▂▁▁▁▁</td></tr><tr><td>throughput/device/flops_per_sec</td><td>█▇▆▆▆▆▅▄▄▅▅▅▆▄▄▅▄▄▄▃▃▃▄▄▃▄▄▅▄▅▄▂▁▂▃▂▁▁▁▁</td></tr><tr><td>throughput/device/mfu</td><td>█▇▆▆▆▆▅▄▄▅▅▅▆▄▄▅▄▄▄▃▃▃▄▄▃▄▄▅▄▅▄▂▁▂▃▂▁▁▁▁</td></tr><tr><td>throughput/device/samples_per_sec</td><td>█▇▆▆▆▆▅▄▄▅▅▅▆▄▄▅▄▄▄▃▃▃▄▄▃▄▄▅▄▅▄▂▁▂▃▂▁▁▁▁</td></tr><tr><td>throughput/flops_per_sec</td><td>█▇▆▆▆▆▅▄▄▅▅▅▆▄▄▅▄▄▄▃▃▃▄▄▃▄▄▅▄▅▄▂▁▂▃▂▁▁▁▁</td></tr><tr><td>throughput/samples_per_sec</td><td>█▇▆▆▆▆▅▄▄▅▅▅▆▄▄▅▄▄▄▃▃▃▄▄▃▄▄▅▄▅▄▂▁▂▃▂▁▁▁▁</td></tr><tr><td>time/batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/batch_in_epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/epoch</td><td>▁█</td></tr><tr><td>time/sample</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/sample_in_epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/token</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/token_in_epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/total</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>time/train</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>time/val</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/device_train_microbatch_size</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss/train/total</td><td>6.01482</td></tr><tr><td>metrics/train/LanguageCrossEntropy</td><td>6.01482</td></tr><tr><td>throughput/batches_per_sec</td><td>9.39411</td></tr><tr><td>throughput/device/batches_per_sec</td><td>9.39411</td></tr><tr><td>throughput/device/flops_per_sec</td><td>618565059620.1593</td></tr><tr><td>throughput/device/mfu</td><td>0.07637</td></tr><tr><td>throughput/device/samples_per_sec</td><td>595.58629</td></tr><tr><td>throughput/flops_per_sec</td><td>618565059620.1593</td></tr><tr><td>throughput/samples_per_sec</td><td>595.58629</td></tr><tr><td>time/batch</td><td>2458</td></tr><tr><td>time/batch_in_epoch</td><td>0</td></tr><tr><td>time/epoch</td><td>1</td></tr><tr><td>time/sample</td><td>157252</td></tr><tr><td>time/sample_in_epoch</td><td>0</td></tr><tr><td>time/token</td><td>40255488</td></tr><tr><td>time/token_in_epoch</td><td>40255488</td></tr><tr><td>time/total</td><td>0.07271</td></tr><tr><td>time/train</td><td>0.07271</td></tr><tr><td>time/val</td><td>0.0</td></tr><tr><td>trainer/device_train_microbatch_size</td><td>64</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">1687841559-adaptable-cricket</strong> at: <a href='https://wandb.ai/john-sungjin/wabisabi/runs/z4zgh8xz' target=\"_blank\">https://wandb.ai/john-sungjin/wabisabi/runs/z4zgh8xz</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230627_050341-z4zgh8xz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=composer_model,  # This is the model from the HuggingFaceModel wrapper class.\n",
    "    train_dataloader=train_dataloader,\n",
    "    # eval_dataloader=eval_dataloader,\n",
    "    max_duration=\"1ep\",  # train for more epochs to get better performance\n",
    "    optimizers=optimizer,\n",
    "    schedulers=[lr_scheduler],\n",
    "    device=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    precision=\"fp32\",\n",
    "    progress_bar=True,\n",
    "    loggers=[wandb_logger],\n",
    "    callbacks=[SpeedMonitor()],\n",
    "    # checkpointing\n",
    "    save_folder=save_folder,\n",
    "    save_filename=\"ep{epoch}-ba{batch}-rank{rank}.pt\",\n",
    "    save_interval=save_interval,\n",
    "    save_overwrite=True,\n",
    ")\n",
    "try:\n",
    "    # Start training\n",
    "    trainer.fit()\n",
    "\n",
    "    # Save Hugging Face model\n",
    "    config.save_pretrained(hf_save_folder)\n",
    "    tokenizer.save_pretrained(hf_save_folder)\n",
    "    composer_model.model.save_pretrained(hf_save_folder)\n",
    "finally:\n",
    "    trainer.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "Wanted to compare Mosaic's estimate with the deepspeed profiler. Looks about right - the MPT estimate is 66 G flops, while the deepspeed estimates 20.03 G flops for forward inference, which means total flops would be 20.03 * 3 = 60.09 G flops. Pretty close!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610624\n",
      "66.46923264 G\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "mpt_estimate = composer_model.flops_per_batch(batch)\n",
    "print(composer_model.n_active_params)\n",
    "print(mpt_estimate / 1e9, \"G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0980e+00, -1.1528e-02, -7.1528e-01,  ...,  1.5902e+00,\n",
       "           1.2393e+00,  1.0311e+00],\n",
       "         [ 7.4352e-01, -1.4786e+00,  3.3844e-01,  ...,  1.5986e+00,\n",
       "          -5.4977e-01, -1.1131e+00],\n",
       "         [ 9.5008e-01, -1.1936e+00, -1.1842e-01,  ..., -3.4778e-01,\n",
       "           3.1346e-01,  1.3806e-01],\n",
       "         ...,\n",
       "         [-3.6953e-02,  5.9130e-01,  6.3312e-01,  ..., -6.7467e-01,\n",
       "           1.1711e+00, -1.8665e-01],\n",
       "         [-1.2335e+00,  2.2180e-01,  6.8240e-01,  ..., -1.5828e-01,\n",
       "          -9.0393e-01, -4.7375e-01],\n",
       "         [ 6.5973e-01,  4.2160e-01, -8.4262e-01,  ..., -1.2005e-01,\n",
       "          -3.9789e-01,  5.5433e-01]],\n",
       "\n",
       "        [[-1.9193e-01, -7.7889e-01,  4.1020e-01,  ...,  8.7949e-01,\n",
       "           7.6157e-01, -1.2559e+00],\n",
       "         [ 3.5590e-01, -9.7230e-01,  3.1907e-01,  ..., -1.5804e+00,\n",
       "          -1.5815e+00, -4.3272e-01],\n",
       "         [-1.0980e+00, -1.1528e-02, -7.1528e-01,  ...,  1.5902e+00,\n",
       "           1.2393e+00,  1.0311e+00],\n",
       "         ...,\n",
       "         [ 7.1277e-01, -2.2877e+00,  4.3686e-01,  ...,  1.0806e-01,\n",
       "           1.3275e+00, -4.1863e-01],\n",
       "         [-1.1362e-01, -7.6255e-01, -8.3364e-01,  ..., -8.5489e-01,\n",
       "           1.5042e+00,  7.1609e-01],\n",
       "         [-4.1291e-01, -1.7300e+00, -6.4006e-01,  ..., -1.3567e+00,\n",
       "          -5.2061e-01,  3.4690e-01]],\n",
       "\n",
       "        [[ 9.1169e-01, -2.1514e+00, -5.7449e-01,  ..., -2.9918e-01,\n",
       "          -4.8054e-01,  8.1013e-01],\n",
       "         [-1.7040e+00,  8.8942e-01,  2.7634e-01,  ..., -4.4354e-01,\n",
       "           1.0497e+00,  8.8926e-01],\n",
       "         [-1.5689e+00, -1.1515e+00, -1.0705e-01,  ...,  2.3685e-01,\n",
       "          -9.5182e-01, -4.5237e-01],\n",
       "         ...,\n",
       "         [ 2.8332e-01, -2.0793e-01,  5.3289e-01,  ...,  1.8034e+00,\n",
       "           1.1254e+00,  1.8468e-01],\n",
       "         [-2.4604e+00, -2.8126e-01,  2.4597e+00,  ...,  3.1986e-02,\n",
       "           1.2023e+00, -3.7289e-01],\n",
       "         [ 8.1321e-01,  2.2483e-01,  2.9953e-01,  ...,  1.4426e+00,\n",
       "          -1.2381e+00,  2.0790e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 8.1321e-01,  2.2483e-01,  2.9953e-01,  ...,  1.4426e+00,\n",
       "          -1.2381e+00,  2.0790e-01],\n",
       "         [-2.1598e+00, -2.5319e+00, -2.4515e+00,  ...,  1.6278e+00,\n",
       "           3.3929e-01, -1.8120e+00],\n",
       "         [ 8.7907e-02,  7.3106e-01,  5.1090e-01,  ...,  9.0888e-01,\n",
       "           1.8439e+00,  5.1943e-01],\n",
       "         ...,\n",
       "         [-1.1361e+00, -1.1130e+00,  1.6512e+00,  ..., -1.0518e-01,\n",
       "          -4.8942e-01, -1.9736e+00],\n",
       "         [-1.1361e+00, -1.1130e+00,  1.6512e+00,  ..., -1.0518e-01,\n",
       "          -4.8942e-01, -1.9736e+00],\n",
       "         [-1.1361e+00, -1.1130e+00,  1.6512e+00,  ..., -1.0518e-01,\n",
       "          -4.8942e-01, -1.9736e+00]],\n",
       "\n",
       "        [[ 4.8071e-01, -1.7694e+00,  7.0750e-01,  ...,  9.1336e-01,\n",
       "           3.0543e-01,  5.6635e-02],\n",
       "         [ 4.3200e-01,  3.6622e-01,  2.9215e+00,  ..., -3.5052e-01,\n",
       "          -1.8450e-01,  1.8383e+00],\n",
       "         [ 3.1996e-02, -9.3292e-01, -1.3948e+00,  ..., -1.4074e+00,\n",
       "          -1.0812e-01,  2.7902e-01],\n",
       "         ...,\n",
       "         [ 4.6799e-01, -2.0546e+00, -3.6393e-01,  ...,  1.3694e+00,\n",
       "           2.2447e-01,  7.7585e-01],\n",
       "         [ 6.6864e-01,  1.5483e+00,  1.2140e+00,  ..., -2.0154e+00,\n",
       "           2.0812e+00,  3.8715e-01],\n",
       "         [ 3.3191e-01, -1.1434e+00,  2.8706e-01,  ...,  4.7930e-01,\n",
       "          -1.1064e+00,  2.5995e-01]],\n",
       "\n",
       "        [[-2.9127e-01,  1.1602e+00,  1.2354e+00,  ...,  9.7506e-02,\n",
       "          -4.3250e-01,  8.9703e-01],\n",
       "         [ 4.1365e-01, -1.3331e+00, -2.9540e-01,  ...,  5.6800e-01,\n",
       "          -4.0983e-01, -9.9730e-01],\n",
       "         [ 3.3191e-01, -1.1434e+00,  2.8706e-01,  ...,  4.7930e-01,\n",
       "          -1.1064e+00,  2.5995e-01],\n",
       "         ...,\n",
       "         [ 1.3088e+00,  5.7905e-01,  5.7377e-01,  ..., -1.2820e+00,\n",
       "           1.3974e+00,  7.8925e-01],\n",
       "         [ 6.1847e-02,  2.3352e+00,  1.0211e-01,  ...,  3.9881e-01,\n",
       "          -2.0909e-01,  6.5620e-01],\n",
       "         [-4.3358e-01, -9.0980e-01,  1.1175e+00,  ..., -2.1903e+00,\n",
       "          -8.1773e-04,  4.1227e-01]]], device='cuda:0',\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = torch.nn.Embedding(\n",
    "    num_embeddings=config.vocab_size, embedding_dim=config.d_model\n",
    ").to(\"cuda\")\n",
    "emb(batch[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cpu\n",
      "torch.int64\n",
      "cpu\n",
      "torch.int64\n",
      "\n",
      "-------------------------- DeepSpeed Flops Profiler --------------------------\n",
      "Profile Summary at step 1:\n",
      "Notations:\n",
      "data parallel size (dp_size), model parallel size(mp_size),\n",
      "number of parameters (params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (flops), floating-point operations per second (FLOPS),\n",
      "fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n",
      "step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n",
      "\n",
      "params per gpu:                                               610.62 k\n",
      "params of model = params per GPU * mp_size:                   0       \n",
      "fwd MACs per GPU:                                             10.0 GMACs\n",
      "fwd flops per GPU:                                            20.03 G \n",
      "fwd flops of model = fwd flops per GPU * mp_size:             20.03 G \n",
      "fwd latency:                                                  109.0 ms\n",
      "fwd FLOPS per GPU = fwd flops per GPU / fwd latency:          183.78 GFLOPS\n",
      "\n",
      "----------------------------- Aggregated Profile per GPU -----------------------------\n",
      "Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n",
      "depth 0:\n",
      "    params      - {'WSModel': '610.62 k'}\n",
      "    MACs        - {'WSModel': '10.0 GMACs'}\n",
      "    fwd latency - {'WSModel': '109.0 ms'}\n",
      "depth 1:\n",
      "    params      - {'Embedding': '524.29 k'}\n",
      "    MACs        - {'Linear': '8.59 GMACs'}\n",
      "    fwd latency - {'Linear': '77.58 ms'}\n",
      "depth 2:\n",
      "    params      - {'WSBlock': '86.27 k'}\n",
      "    MACs        - {'WSBlock': '1.41 GMACs'}\n",
      "    fwd latency - {'WSBlock': '30.04 ms'}\n",
      "depth 3:\n",
      "    params      - {'FusedMLP': '65.54 k'}\n",
      "    MACs        - {'FusedMLP': '1.07 GMACs'}\n",
      "    fwd latency - {'FusedMLP': '17.29 ms'}\n",
      "depth 4:\n",
      "    params      - {'Sequential': '65.54 k'}\n",
      "    MACs        - {'Sequential': '1.07 GMACs'}\n",
      "    fwd latency - {'Sequential': '17.22 ms'}\n",
      "depth 5:\n",
      "    params      - {'Linear': '65.54 k'}\n",
      "    MACs        - {'Linear': '1.07 GMACs'}\n",
      "    fwd latency - {'Linear': '11.77 ms'}\n",
      "\n",
      "------------------------------ Detailed Profile per GPU ------------------------------\n",
      "Each module profile is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n",
      "\n",
      "WSModel(\n",
      "  610.62 k, 100.00% Params, 10.0 GMACs, 100.00% MACs, 109.0 ms, 100.00% latency, 183.78 GFLOPS, \n",
      "  (tokens_to_embeddings): Embedding(524.29 k, 85.86% Params, 0 MACs, 0.00% MACs, 244.38 us, 0.22% latency, 0.0 FLOPS, 8192, 64)\n",
      "  (blocks): ModuleList(\n",
      "    (0): WSBlock(\n",
      "      43.14 k, 7.06% Params, 704.64 MMACs, 7.05% MACs, 14.98 ms, 13.74% latency, 95.05 GFLOPS, \n",
      "      (layer_norm_before_attention): FusedLayerNorm(64, 0.01% Params, 0 MACs, 0.00% MACs, 354.77 us, 0.33% latency, 14.78 GFLOPS, )\n",
      "      (attention): WSMultiQueryAttention(\n",
      "        10.24 k, 1.68% Params, 167.77 MMACs, 1.68% MACs, 5.18 ms, 4.75% latency, 64.76 GFLOPS, \n",
      "        (residual_to_qkv): Linear(6.14 k, 1.01% Params, 100.66 MMACs, 1.01% MACs, 1.35 ms, 1.24% latency, 149.38 GFLOPS, in_features=64, out_features=96, bias=False)\n",
      "        (concat_attention_to_residual): Linear(4.1 k, 0.67% Params, 67.11 MMACs, 0.67% MACs, 783.44 us, 0.72% latency, 171.32 GFLOPS, in_features=64, out_features=64, bias=False)\n",
      "      )\n",
      "      (layer_norm_before_ffn): FusedLayerNorm(64, 0.01% Params, 0 MACs, 0.00% MACs, 362.87 us, 0.33% latency, 14.45 GFLOPS, )\n",
      "      (ffn): FusedMLP(\n",
      "        32.77 k, 5.37% Params, 536.87 MMACs, 5.37% MACs, 8.46 ms, 7.76% latency, 127.36 GFLOPS, \n",
      "        (mlp): Sequential(\n",
      "          32.77 k, 5.37% Params, 536.87 MMACs, 5.37% MACs, 8.43 ms, 7.73% latency, 127.92 GFLOPS, \n",
      "          (0): Linear(16.38 k, 2.68% Params, 268.44 MMACs, 2.68% MACs, 2.56 ms, 2.35% latency, 210.0 GFLOPS, in_features=64, out_features=256, bias=False)\n",
      "          (1): FusedDropoutBias(\n",
      "            0, 0.00% Params, 0 MACs, 0.00% MACs, 1.86 ms, 1.71% latency, 2.25 GFLOPS, \n",
      "            (activation_pytorch): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 1.04 ms, 0.95% latency, 4.04 GFLOPS, approximate='none')\n",
      "          )\n",
      "          (2): Linear(16.38 k, 2.68% Params, 268.44 MMACs, 2.68% MACs, 3.64 ms, 3.34% latency, 147.31 GFLOPS, in_features=256, out_features=64, bias=False)\n",
      "          (3): FusedDropoutBias(\n",
      "            0, 0.00% Params, 0 MACs, 0.00% MACs, 255.35 us, 0.23% latency, 0.0 FLOPS, \n",
      "            (activation_pytorch): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 16.45 us, 0.02% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): WSBlock(\n",
      "      43.14 k, 7.06% Params, 704.64 MMACs, 7.05% MACs, 15.06 ms, 13.81% latency, 94.56 GFLOPS, \n",
      "      (layer_norm_before_attention): FusedLayerNorm(64, 0.01% Params, 0 MACs, 0.00% MACs, 373.13 us, 0.34% latency, 14.05 GFLOPS, )\n",
      "      (attention): WSMultiQueryAttention(\n",
      "        10.24 k, 1.68% Params, 167.77 MMACs, 1.68% MACs, 4.92 ms, 4.52% latency, 68.15 GFLOPS, \n",
      "        (residual_to_qkv): Linear(6.14 k, 1.01% Params, 100.66 MMACs, 1.01% MACs, 1.21 ms, 1.11% latency, 166.45 GFLOPS, in_features=64, out_features=96, bias=False)\n",
      "        (concat_attention_to_residual): Linear(4.1 k, 0.67% Params, 67.11 MMACs, 0.67% MACs, 787.02 us, 0.72% latency, 170.54 GFLOPS, in_features=64, out_features=64, bias=False)\n",
      "      )\n",
      "      (layer_norm_before_ffn): FusedLayerNorm(64, 0.01% Params, 0 MACs, 0.00% MACs, 367.4 us, 0.34% latency, 14.27 GFLOPS, )\n",
      "      (ffn): FusedMLP(\n",
      "        32.77 k, 5.37% Params, 536.87 MMACs, 5.37% MACs, 8.83 ms, 8.10% latency, 122.08 GFLOPS, \n",
      "        (mlp): Sequential(\n",
      "          32.77 k, 5.37% Params, 536.87 MMACs, 5.37% MACs, 8.79 ms, 8.07% latency, 122.6 GFLOPS, \n",
      "          (0): Linear(16.38 k, 2.68% Params, 268.44 MMACs, 2.68% MACs, 2.36 ms, 2.17% latency, 227.04 GFLOPS, in_features=64, out_features=256, bias=False)\n",
      "          (1): FusedDropoutBias(\n",
      "            0, 0.00% Params, 0 MACs, 0.00% MACs, 2.83 ms, 2.60% latency, 1.48 GFLOPS, \n",
      "            (activation_pytorch): GELU(0, 0.00% Params, 0 MACs, 0.00% MACs, 1.54 ms, 1.41% latency, 2.73 GFLOPS, approximate='none')\n",
      "          )\n",
      "          (2): Linear(16.38 k, 2.68% Params, 268.44 MMACs, 2.68% MACs, 3.21 ms, 2.94% latency, 167.3 GFLOPS, in_features=256, out_features=64, bias=False)\n",
      "          (3): FusedDropoutBias(\n",
      "            0, 0.00% Params, 0 MACs, 0.00% MACs, 271.8 us, 0.25% latency, 0.0 FLOPS, \n",
      "            (activation_pytorch): Identity(0, 0.00% Params, 0 MACs, 0.00% MACs, 16.69 us, 0.02% latency, 0.0 FLOPS, )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer_norm_final): FusedLayerNorm(64, 0.01% Params, 0 MACs, 0.00% MACs, 863.08 us, 0.79% latency, 6.07 GFLOPS, )\n",
      "  (embeddings_to_logits): Linear(524.29 k, 85.86% Params, 8.59 GMACs, 85.91% MACs, 77.58 ms, 71.17% latency, 221.45 GFLOPS, in_features=64, out_features=8192, bias=False)\n",
      ")\n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from deepspeed.profiling.flops_profiler import get_model_profile\n",
    "\n",
    "batch = next(iter(train_dataloader)).to(\"cuda\")\n",
    "batch_size = 64\n",
    "test_composer_model = ComposerWSModel(config=config, tokenizer=tokenizer)\n",
    "print(test_composer_model.model.device)\n",
    "flops, macs, params = get_model_profile(\n",
    "    model=test_composer_model.model,\n",
    "    input_shape=(batch_size, 256),\n",
    "    # kwargs={\"input_ids\": batch[\"input_ids\"]},\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
