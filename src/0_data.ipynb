{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizerFast\n",
    "from composer.models.huggingface import HuggingFaceModel\n",
    "\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2965947075.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    def __init__(self, config: WabiSabiConfig):\u001b[0m\n\u001b[0m                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class WabiSabiConfig:\n",
    "    \"\"\"\n",
    "    Important ratios:\n",
    "    - d_model should be a multiple of n_heads\n",
    "    - d_q, d_k, d_v are all equal to d_model / n_heads\n",
    "    \"\"\"\n",
    "\n",
    "    d_model: int = 2048\n",
    "    n_heads: int = 16\n",
    "    n_layers: int = 24\n",
    "    vocab_size: int = 50368\n",
    "\n",
    "\n",
    "# Flash attention, multi-query attention\n",
    "class WabiSabiBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layer_norm_before_attention = nn.LayerNorm(d_model)\n",
    "\n",
    "\n",
    "class WabiSabiModel(PreTrainedModel):\n",
    "    def __init__(self, config: WabiSabiConfig):\n",
    "        self.config = config\n",
    "        self.tokens_to_embeddings = nn.Embedding(\n",
    "            num_embeddings=config.vocab_size, embedding_dim=config.d_model\n",
    "        )\n",
    "        self.embeddings_to_logits = nn.Linear(\n",
    "            in_features=config.d_model, out_features=config.vocab_size\n",
    "        )\n",
    "\n",
    "        # https://paperswithcode.com/method/weight-tying\n",
    "        self.embeddings_to_logits.weight = self.tokens_to_embeddings.weight\n",
    "\n",
    "        # Turned into low precision layernorm by composer\n",
    "        # https://docs.mosaicml.com/projects/composer/en/latest/method_cards/low_precision_layernorm.html\n",
    "        self.layer_norm_final = nn.LayerNorm(config.d_model)\n",
    "\n",
    "        # Embedding fraction: page 7 of GLM-130B paper https://arxiv.org/abs/2210.02414\n",
    "        # self.embedding_fraction = config.embedding_fraction\n",
    "\n",
    "        # initialize parameters\n",
    "        # notes from MPT/nanoGPT/transformers\n",
    "        # 1. residual projections (e.g. linear layers that project to d_model) are divided\n",
    "        # by 1 / sqrt(num_layers)\n",
    "        # 2. layer norm weights are set to one (PyTorch sets this by default; skip)\n",
    "        # 3. all others are initialized with normal distribution with mean 0 and std 0.02\n",
    "        # Note: MPT uses kaiming_normal; I'll go for this as well\n",
    "        def init_weights(module: nn.Module):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight)\n",
    "                if module._is_residual_projection:\n",
    "                    with torch.no_grad():\n",
    "                        module.weight.div_(torch.sqrt(config.n_layers))\n",
    "\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.kaiming_normal_(module.weight)\n",
    "\n",
    "        # disable bias in all modules\n",
    "        # note for later: if you want to enable bias, should remember to zero out all biases\n",
    "        # in init_weights\n",
    "        def disable_bias(module: nn.Module):\n",
    "            if hasattr(module, \"bias\") and isinstance(module.bias, nn.Parameter):\n",
    "                module.register_parameter(\"bias\", None)\n",
    "\n",
    "        self.apply(init_weights)\n",
    "        self.apply(disable_bias)\n",
    "\n",
    "    # TODO: kwargs are for other HuggingFace generate params. Implement if needed.\n",
    "    def forward(self, input_ids: torch.LongTensor, **kwargs):\n",
    "        x = self.tokens_to_embeddings(input_ids)\n",
    "\n",
    "        # MPT doesn't use embedding fraction\n",
    "        # x = (x * self.embedding_fraction) + (x.detach() * (1 - self.embedding_fraction))\n",
    "        # blocks\n",
    "\n",
    "        x = self.layer_norm_final(x)\n",
    "        x = self.embeddings_to_logits(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHILDREN\n",
      "0 Linear(in_features=10, out_features=10, bias=True)\n",
      "1 ModuleList(\n",
      "  (0-9): 10 x Linear(in_features=10, out_features=10, bias=True)\n",
      ")\n",
      "MODULES\n",
      "0 TestModule(\n",
      "  (linear): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (linear_list): ModuleList(\n",
      "    (0-9): 10 x Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "1 Linear(in_features=10, out_features=10, bias=True)\n",
      "2 ModuleList(\n",
      "  (0-9): 10 x Linear(in_features=10, out_features=10, bias=True)\n",
      ")\n",
      "3 Linear(in_features=10, out_features=10, bias=True)\n",
      "4 Linear(in_features=10, out_features=10, bias=True)\n",
      "5 Linear(in_features=10, out_features=10, bias=True)\n",
      "6 Linear(in_features=10, out_features=10, bias=True)\n",
      "7 Linear(in_features=10, out_features=10, bias=True)\n",
      "8 Linear(in_features=10, out_features=10, bias=True)\n",
      "9 Linear(in_features=10, out_features=10, bias=True)\n",
      "10 Linear(in_features=10, out_features=10, bias=True)\n",
      "11 Linear(in_features=10, out_features=10, bias=True)\n",
      "12 Linear(in_features=10, out_features=10, bias=True)\n",
      "APPLY\n",
      "APPLY\n",
      "APPLY\n",
      "APPLY\n",
      "APPLY\n",
      "APPLY\n",
      "APPLY\n",
      "APPLY\n",
      "APPLY\n",
      "APPLY\n",
      "APPLY\n",
      "APPLY\n",
      "APPLY\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TestModule(\n",
       "  (linear): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (linear_list): ModuleList(\n",
       "    (0-9): 10 x Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apply_fn(module: nn.Module):\n",
    "    print(\"APPLY\")\n",
    "\n",
    "\n",
    "class TestModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(10, 10)\n",
    "        self.linear_list = nn.ModuleList([nn.Linear(10, 10) for _ in range(10)])\n",
    "\n",
    "        print(\"CHILDREN\")\n",
    "        for ind, child in enumerate(self.children()):\n",
    "            print(ind, child)\n",
    "\n",
    "        print(\"MODULES\")\n",
    "        for ind, module in enumerate(self.modules()):\n",
    "            print(ind, module)\n",
    "\n",
    "        self.apply(apply_fn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "TestModule()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
