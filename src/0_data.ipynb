{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizerFast, PretrainedConfig\n",
    "from composer.models.huggingface import HuggingFaceModel\n",
    "\n",
    "from xformers.components.feedforward import FusedMLP\n",
    "from xformers.triton import FusedLayerNorm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "# need to fix this config\n",
    "class WabiSabiConfig(PretrainedConfig):\n",
    "    \"\"\"\n",
    "    Important ratios:\n",
    "    - d_model should be a multiple of n_heads\n",
    "    - d_q, d_k, d_v are all equal to d_model / n_heads\n",
    "    \"\"\"\n",
    "\n",
    "    d_model: int = 2048\n",
    "    n_heads: int = 16\n",
    "    n_layers: int = 24\n",
    "    vocab_size: int = 50368\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.d_head = self.d_model // self.n_heads\n",
    "\n",
    "\n",
    "class WSMultiQueryAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, d_head: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # NOTE: this layer concatenates several tensors together\n",
    "        # When initializing params, we should instantiate these separately.\n",
    "        # output size: q -> d_model, k and v -> head_dim\n",
    "        self.residual_to_qkv = nn.Linear(d_model, d_model + 2 * d_head)\n",
    "\n",
    "        self.concat_attention_to_residual = nn.Linear(d_model, d_model)\n",
    "        self.concat_attention_to_residual._is_residual_projection = True\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        qkv = self.residual_to_qkv(x)\n",
    "        # split qkv into q, k, v\n",
    "        q, k, v = torch.split(qkv, [self.d_model, self.d_head, self.d_head], dim=-1)\n",
    "\n",
    "        # PyTorch's flash attention implementation\n",
    "        attention = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        x = self.concat_attention_to_residual(attention)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Flash attention, multi-query attention\n",
    "class WSBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_head: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layer_norm_before_attention = FusedLayerNorm(d_model)\n",
    "        self.attention = WSMultiQueryAttention(d_model, d_head)\n",
    "        self.layer_norm_before_ffn = FusedLayerNorm(d_model)\n",
    "\n",
    "        # this ratio is standard for transformers\n",
    "        # Triton fused MLP. The linear layers in PyTorch are already fused,\n",
    "        # but xformers has a custom implementation that fuses dropout and bias\n",
    "        ffn_ratio = 4\n",
    "        self.ffn = FusedMLP(\n",
    "            dim_model=d_model,\n",
    "            dropout=0.0,\n",
    "            activation=\"gelu\",\n",
    "            hidden_layer_multiplier=ffn_ratio,\n",
    "        )\n",
    "        # index of the last linear layer\n",
    "        self.ffn.mlp[2]._is_residual_projection = True\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x_norm = self.layer_norm_before_attention(x)\n",
    "        x_attn = self.attention(x_norm)\n",
    "        x = x + x_attn\n",
    "\n",
    "        x_norm = self.layer_norm_before_ffn(x)\n",
    "        x_ffn = self.ffn(x_norm)\n",
    "        x = x + x_ffn\n",
    "        return x\n",
    "\n",
    "\n",
    "class WabiSabiModel(PreTrainedModel):\n",
    "    def __init__(self, config: WabiSabiConfig):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.tokens_to_embeddings = nn.Embedding(\n",
    "            num_embeddings=config.vocab_size, embedding_dim=config.d_model\n",
    "        )\n",
    "\n",
    "        # Embedding fraction: page 7 of GLM-130B paper https://arxiv.org/abs/2210.02414\n",
    "        # self.embedding_fraction = config.embedding_fraction\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                WSBlock(\n",
    "                    d_model=config.d_model,\n",
    "                    d_head=config.d_head,\n",
    "                )\n",
    "                for _ in range(config.n_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Could also consider the Mosaic implementation...\n",
    "        # https://docs.mosaicml.com/projects/composer/en/latest/method_cards/low_precision_layernorm.html\n",
    "        self.layer_norm_final = FusedLayerNorm(config.d_model)\n",
    "\n",
    "        self.embeddings_to_logits = nn.Linear(\n",
    "            in_features=config.d_model, out_features=config.vocab_size\n",
    "        )\n",
    "\n",
    "        # https://paperswithcode.com/method/weight-tying\n",
    "        self.embeddings_to_logits.weight = self.tokens_to_embeddings.weight\n",
    "\n",
    "        # initialize parameters\n",
    "        # notes from MPT/nanoGPT/transformers\n",
    "        # 1. residual projections (e.g. linear layers that project to d_model) are divided\n",
    "        # by 1 / sqrt(num_layers)\n",
    "        # 2. layer norm weights are set to one (PyTorch sets this by default; skip)\n",
    "        # 3. all others are initialized with normal distribution with mean 0 and std 0.02\n",
    "        # Note: MPT uses kaiming_normal; I'll go for this as well\n",
    "        def init_weights(module: nn.Module):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight)\n",
    "                if module._is_residual_projection:\n",
    "                    with torch.no_grad():\n",
    "                        module.weight.div_(torch.sqrt(config.n_layers))\n",
    "\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.kaiming_normal_(module.weight)\n",
    "\n",
    "        # disable bias in all modules\n",
    "        # note for later: if you want to enable bias, should remember to zero out all biases\n",
    "        # in init_weights\n",
    "        def disable_bias(module: nn.Module):\n",
    "            if hasattr(module, \"bias\") and isinstance(module.bias, nn.Parameter):\n",
    "                module.register_parameter(\"bias\", None)\n",
    "\n",
    "        self.apply(init_weights)\n",
    "        self.apply(disable_bias)\n",
    "\n",
    "    # TODO: kwargs are for other HuggingFace generate params. Implement if needed.\n",
    "    def forward(self, input_ids: torch.LongTensor, **kwargs):\n",
    "        x = self.tokens_to_embeddings(input_ids)\n",
    "\n",
    "        # MPT doesn't use embedding fraction\n",
    "        # x = (x * self.embedding_fraction) + (x.detach() * (1 - self.embedding_fraction))\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.layer_norm_final(x)\n",
    "        x = self.embeddings_to_logits(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "cannot assign module before Module.__init__() call",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m WabiSabiModel(WabiSabiConfig())\n",
      "Cell \u001b[0;32mIn[5], line 80\u001b[0m, in \u001b[0;36mWabiSabiModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config: WabiSabiConfig):\n\u001b[1;32m     79\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m config\n\u001b[0;32m---> 80\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokens_to_embeddings \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(\n\u001b[1;32m     81\u001b[0m         num_embeddings\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mvocab_size, embedding_dim\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39md_model\n\u001b[1;32m     82\u001b[0m     )\n\u001b[1;32m     84\u001b[0m     \u001b[39m# Embedding fraction: page 7 of GLM-130B paper https://arxiv.org/abs/2210.02414\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[39m# self.embedding_fraction = config.embedding_fraction\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList(\n\u001b[1;32m     88\u001b[0m         [\n\u001b[1;32m     89\u001b[0m             WSBlock(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m         ]\n\u001b[1;32m     95\u001b[0m     )\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1643\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1641\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, Module):\n\u001b[1;32m   1642\u001b[0m     \u001b[39mif\u001b[39;00m modules \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1643\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m   1644\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mcannot assign module before Module.__init__() call\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1645\u001b[0m     remove_from(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parameters, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_non_persistent_buffers_set)\n\u001b[1;32m   1646\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m _global_module_registration_hooks\u001b[39m.\u001b[39mvalues():\n",
      "\u001b[0;31mAttributeError\u001b[0m: cannot assign module before Module.__init__() call"
     ]
    }
   ],
   "source": [
    "model = WabiSabiModel(WabiSabiConfig())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
