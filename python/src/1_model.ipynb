{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Either FairScale or torch distributed is not available, MixtureOfExperts will not be exposed. Please install them if you would like to use MoE\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedModel, PreTrainedTokenizerFast, PretrainedConfig\n",
    "from composer.models.huggingface import HuggingFaceModel\n",
    "\n",
    "from xformers.components.feedforward import FusedMLP\n",
    "from xformers.triton import FusedLayerNorm\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchinfo import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to fix this config\n",
    "class WabiSabiConfig(PretrainedConfig):\n",
    "    \"\"\"\n",
    "    Important ratios:\n",
    "    - d_model should be a multiple of n_heads\n",
    "    - d_q, d_k, d_v are all equal to d_model / n_heads\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 64,\n",
    "        n_heads: int = 2,\n",
    "        n_layers: int = 2,\n",
    "        vocab_size: int = 50368,\n",
    "        **kwargs,  # for other HuggingFace params\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_head = self.d_model // self.n_heads\n",
    "\n",
    "\n",
    "class WSMultiQueryAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, d_head: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # NOTE: this layer concatenates several tensors together\n",
    "        # When initializing params, we should instantiate these separately.\n",
    "        # output size: q -> d_model, k and v -> head_dim\n",
    "        self.residual_to_qkv = nn.Linear(d_model, d_model + 2 * d_head)\n",
    "        # _splits[0]: the dimension by which the tensor is split\n",
    "        # Note that, for nn.Linear(dim_a, dim_b), the weights are of shape (dim_b, dim_a)\n",
    "        # So, the split dimension here is 0\n",
    "        # _splits[1]: the split sizes\n",
    "        self.residual_to_qkv._splits = (0, (d_model, d_head, d_head))\n",
    "\n",
    "        self.concat_attention_to_residual = nn.Linear(d_model, d_model)\n",
    "        self.concat_attention_to_residual._is_residual_projection = True\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        qkv = self.residual_to_qkv(x)\n",
    "        # split qkv into q, k, v\n",
    "        q, k, v = torch.split(qkv, [self.d_model, self.d_head, self.d_head], dim=-1)\n",
    "\n",
    "        # PyTorch's flash attention implementation\n",
    "        concat_attention = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        x = self.concat_attention_to_residual(concat_attention)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Flash attention, multi-query attention\n",
    "class WSBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_head: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layer_norm_before_attention = FusedLayerNorm(d_model)\n",
    "        self.attention = WSMultiQueryAttention(d_model, d_head)\n",
    "        self.layer_norm_before_ffn = FusedLayerNorm(d_model)\n",
    "\n",
    "        # this ratio is standard for transformers\n",
    "        # Triton fused MLP. The linear layers in PyTorch are already fused,\n",
    "        # but xformers has a custom implementation that fuses dropout and bias\n",
    "        # not useful since there is no bias... but I'll use it anyway\n",
    "        ffn_ratio = 4\n",
    "        self.ffn = FusedMLP(\n",
    "            dim_model=d_model,\n",
    "            dropout=0.0,\n",
    "            activation=\"gelu\",\n",
    "            hidden_layer_multiplier=ffn_ratio,\n",
    "        )\n",
    "        # index of the last linear layer\n",
    "        self.ffn.mlp[2]._is_residual_projection = True\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x_norm = self.layer_norm_before_attention(x)\n",
    "        x_attn = self.attention(x_norm)\n",
    "        x = x + x_attn\n",
    "\n",
    "        x_norm = self.layer_norm_before_ffn(x)\n",
    "        x_ffn = self.ffn(x_norm)\n",
    "        x = x + x_ffn\n",
    "        return x\n",
    "\n",
    "\n",
    "class WabiSabiModel(PreTrainedModel):\n",
    "    def __init__(self, config: WabiSabiConfig):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.tokens_to_embeddings = nn.Embedding(\n",
    "            num_embeddings=config.vocab_size, embedding_dim=config.d_model\n",
    "        )\n",
    "\n",
    "        # Embedding fraction: page 7 of GLM-130B paper https://arxiv.org/abs/2210.02414\n",
    "        # self.embedding_fraction = config.embedding_fraction\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                WSBlock(\n",
    "                    d_model=config.d_model,\n",
    "                    d_head=config.d_head,\n",
    "                )\n",
    "                for _ in range(config.n_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Could also consider the Mosaic implementation...\n",
    "        # https://docs.mosaicml.com/projects/composer/en/latest/method_cards/low_precision_layernorm.html\n",
    "        self.layer_norm_final = FusedLayerNorm(config.d_model)\n",
    "\n",
    "        self.embeddings_to_logits = nn.Linear(\n",
    "            in_features=config.d_model, out_features=config.vocab_size\n",
    "        )\n",
    "\n",
    "        # https://paperswithcode.com/method/weight-tying\n",
    "        self.embeddings_to_logits.weight = self.tokens_to_embeddings.weight\n",
    "\n",
    "        # initialize parameters\n",
    "        # notes from MPT/nanoGPT/transformers\n",
    "        # 1. residual projections (e.g. linear layers that project to d_model) are divided\n",
    "        # by sqrt(num_layers)\n",
    "        # 2. layer norm weights are set to one (PyTorch sets this by default; skip)\n",
    "        # 3. all others are initialized with normal distribution with mean 0 and std 0.02\n",
    "        # Note: MPT uses kaiming_normal; I'll go for this as well\n",
    "        def init_weights(module: nn.Module):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                if hasattr(module, \"_splits\"):\n",
    "                    split_dim, split_sizes = module._splits\n",
    "                    assert module.weight.shape[split_dim] == sum(split_sizes)\n",
    "                    start = 0\n",
    "                    for size in split_sizes:\n",
    "                        slice_indices = [slice(None)] * module.weight.ndim\n",
    "                        slice_indices[split_dim] = slice(start, start + size)\n",
    "                        nn.init.kaiming_normal_(module.weight[slice_indices])\n",
    "                        start += size\n",
    "                    return\n",
    "\n",
    "                nn.init.kaiming_normal_(module.weight)\n",
    "                if getattr(module, \"_is_residual_projection\", False):\n",
    "                    with torch.no_grad():\n",
    "                        module.weight.div_(math.sqrt(config.n_layers))\n",
    "\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.kaiming_normal_(module.weight)\n",
    "\n",
    "        # disable bias in all modules\n",
    "        # note for later: if you want to enable bias, should remember to zero out all biases\n",
    "        # in init_weights\n",
    "        def disable_bias(module: nn.Module):\n",
    "            if hasattr(module, \"bias\") and isinstance(module.bias, nn.Parameter):\n",
    "                module.register_parameter(\"bias\", None)\n",
    "\n",
    "        self.apply(init_weights)\n",
    "        self.apply(disable_bias)\n",
    "\n",
    "        # print model stats\n",
    "        # num parameters, num flops, num bytes\n",
    "\n",
    "    # TODO: kwargs are for other HuggingFace generate params. Implement if needed.\n",
    "    def forward(self, input_ids: torch.LongTensor, **kwargs):\n",
    "        x = self.tokens_to_embeddings(input_ids)\n",
    "\n",
    "        # MPT doesn't use embedding fraction\n",
    "        # x = (x * self.embedding_fraction) + (x.detach() * (1 - self.embedding_fraction))\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.layer_norm_final(x)\n",
    "        x = self.embeddings_to_logits(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "WabiSabiModel                                      --\n",
       "├─Embedding: 1-1                                   3,223,552\n",
       "├─ModuleList: 1-2                                  --\n",
       "│    └─WSBlock: 2-1                                --\n",
       "│    │    └─FusedLayerNorm: 3-1                    64\n",
       "│    │    └─WSMultiQueryAttention: 3-2             12,288\n",
       "│    │    └─FusedLayerNorm: 3-3                    64\n",
       "│    │    └─FusedMLP: 3-4                          32,768\n",
       "│    └─WSBlock: 2-2                                --\n",
       "│    │    └─FusedLayerNorm: 3-5                    64\n",
       "│    │    └─WSMultiQueryAttention: 3-6             12,288\n",
       "│    │    └─FusedLayerNorm: 3-7                    64\n",
       "│    │    └─FusedMLP: 3-8                          32,768\n",
       "├─FusedLayerNorm: 1-3                              64\n",
       "├─Linear: 1-4                                      3,223,552\n",
       "===========================================================================\n",
       "Total params: 6,537,536\n",
       "Trainable params: 6,537,536\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WabiSabiModel(WabiSabiConfig())\n",
    "summary(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 15.61 GiB total capacity; 14.74 GiB already allocated; 120.75 MiB free; 14.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mcuda()\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    889\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \n\u001b[1;32m    891\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(\u001b[39mlambda\u001b[39;49;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 797 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/datadrive/wabi-sabi/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:905\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    889\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \n\u001b[1;32m    891\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 15.61 GiB total capacity; 14.74 GiB already allocated; 120.75 MiB free; 14.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model.cuda()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
