{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsk/Dev/wabi-sabi/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodel\u001b[39;00m \u001b[39mimport\u001b[39;00m WSConfig, WSModel\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m PreTrainedTokenizerFast, GenerationConfig\n\u001b[1;32m      4\u001b[0m hf_model_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtinystories/runs/ts-1-512seqlen/hfmodel\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/Dev/wabi-sabi/python/src/model.py:19\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     13\u001b[0m     DataCollatorForLanguageModeling,\n\u001b[1;32m     14\u001b[0m     PretrainedConfig,\n\u001b[1;32m     15\u001b[0m     PreTrainedModel,\n\u001b[1;32m     16\u001b[0m     PreTrainedTokenizerFast,\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_outputs\u001b[39;00m \u001b[39mimport\u001b[39;00m CausalLMOutputWithPast\n\u001b[0;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mxformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcomponents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeedforward\u001b[39;00m \u001b[39mimport\u001b[39;00m FusedMLP\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mxformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtriton\u001b[39;00m \u001b[39mimport\u001b[39;00m FusedLayerNorm\n\u001b[1;32m     23\u001b[0m \u001b[39m# need to fix this config\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xformers'"
     ]
    }
   ],
   "source": [
    "from model import WSConfig, WSModel\n",
    "from transformers import PreTrainedTokenizerFast, GenerationConfig\n",
    "\n",
    "hf_model_dir = \"tinystories/runs/ts-1-512seqlen/hfmodel\"\n",
    "\n",
    "test_config = WSConfig.from_pretrained(hf_model_dir)\n",
    "test_model = WSModel.from_pretrained(hf_model_dir, config=test_config)\n",
    "test_tokenizer = PreTrainedTokenizerFast.from_pretrained(hf_model_dir)\n",
    "\n",
    "prompt = \"A Chinese dragon lived in a cave.\"\n",
    "prompt_ids = test_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "print(\"Prompt IDs:\", prompt_ids)\n",
    "\n",
    "\n",
    "output_ids = test_model.generate(\n",
    "    prompt_ids, eos_token_id=test_tokenizer.eos_token_id, max_new_tokens=300\n",
    ")\n",
    "print(\"Output IDs:\", output_ids)\n",
    "\n",
    "output_text = test_tokenizer.decode(output_ids[0])\n",
    "print(\"Output Text:\", output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt IDs: tensor([[ 251, 1782,  560,  346,  313, 5014, 5218,   12,  258, 1513]])\n",
      "Output IDs: tensor([[ 251, 1782,  560,  346,  313, 5014, 5218,   12,  258, 1513,  318,  208,\n",
      "         1181, 3435,  254,  991,  258,  391, 2626,   14,  258,  404,  612,  813,\n",
      "          251,  540,  208, 1181, 3435,  254,  991,  258,  391, 2626,   14,  352,\n",
      "          154,  269,  258,  391,  208, 1181, 3435,  254,  991,  258,  391, 2626,\n",
      "           12,  258,  404,  680,  208,  686,  254,  313, 2521,   14,  154,  269,\n",
      "          258,  391,  208, 1440,   12,  258,  404,  680,  208,  686,  254,  313,\n",
      "         2521,   14,  154,  269,  258,  391,  208, 1440,   12,  258,  404, 1605,\n",
      "          284,  208, 1440,  326, 1699, 2396,   14,  154,  269,  258,  391,  208,\n",
      "         1440,   12,  258,  404, 1605,  284,  208, 1440,  326, 1699, 2396,   14,\n",
      "          154,  269,  258,  391,  208, 1440,   12,  258,  404, 1605,  284,  208,\n",
      "         1440,  326, 1699, 2396,   14,  154,  269,  258,  391,  208, 1440,   12,\n",
      "          258,  404, 1605,  284,  208, 1440,  326, 1699, 2396,   14,  154,  269,\n",
      "          258,  391,  208, 1440,   12,  258,  404, 1605,  284,  208, 1440,  326,\n",
      "         1699, 2396,   14,  154,  269,  258,  391,  208, 1440,   12,  258,  404,\n",
      "         1605,  284,  208, 1440,  326, 1699, 2396,   14,  154,  269,  258,  391,\n",
      "          208, 1440,   12,  258,  404, 1605,  284,  208, 1440,  326, 1699, 2396,\n",
      "           14,  154,  269,  258,  391,  208, 1440,   12,  258,  404, 1605,  284,\n",
      "          208, 1440,  326, 1699, 2396,   14]])\n",
      "Output Text:  to break up with your girlfriend, you must be a good idea of what you are doing. you can also try to get a good idea of what you are doing.\n",
      "\n",
      "\n",
      "if you are a good idea of what you are doing, you can make a list of your friends.\n",
      "if you are a friend, you can make a list of your friends.\n",
      "if you are a friend, you can ask for a friend or family member.\n",
      "if you are a friend, you can ask for a friend or family member.\n",
      "if you are a friend, you can ask for a friend or family member.\n",
      "if you are a friend, you can ask for a friend or family member.\n",
      "if you are a friend, you can ask for a friend or family member.\n",
      "if you are a friend, you can ask for a friend or family member.\n",
      "if you are a friend, you can ask for a friend or family member.\n",
      "if you are a friend, you can ask for a friend or family member.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast, GPTNeoXForCausalLM, GPTNeoXConfig\n",
    "\n",
    "hf_model_dir = \"gpt-neox/huggingface_model/\"\n",
    "\n",
    "test_config = GPTNeoXConfig.from_pretrained(hf_model_dir)\n",
    "test_model = GPTNeoXForCausalLM.from_pretrained(hf_model_dir, config=test_config)\n",
    "test_tokenizer = PreTrainedTokenizerFast.from_pretrained(hf_model_dir)\n",
    "\n",
    "prompt = \"To break up with your girlfriend, you must\"\n",
    "prompt_ids = test_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "print(\"Prompt IDs:\", prompt_ids)\n",
    "\n",
    "\n",
    "output_ids = test_model.generate(prompt_ids, max_new_tokens=200)\n",
    "print(\"Output IDs:\", output_ids)\n",
    "\n",
    "output_text = test_tokenizer.decode(output_ids[0])\n",
    "print(\"Output Text:\", output_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
